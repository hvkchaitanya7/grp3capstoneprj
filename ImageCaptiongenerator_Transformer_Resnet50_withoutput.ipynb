{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ImageCaptiongenerator_Transformer_Resnet50_withoutput.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hvkchaitanya7/grp3capstoneprj/blob/main/ImageCaptiongenerator_Transformer_Resnet50_withoutput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu28IxReZxcR"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "lRwEDVweZawC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download Flickr Data set and unzip it . It has 8000 images and we will be using 5000 for training , 1000 validation and 2000 for testing "
      ],
      "metadata": {
        "id": "VCbRsuWWOYvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Flickr8k_Dataset.zip\n",
        "!rm -rf Flickr8k_text.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip -O Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip -O Flickr8k_text.zip\n",
        "!unzip Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "jXWogC4mZs97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all libraries for Keras , Tensorflow and Resnet50"
      ],
      "metadata": {
        "id": "RoBec8t1Ouoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si32U9x-8wzq"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from numpy import array\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import sys, time, os, warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnpEMWQ6dlA6"
      },
      "source": [
        "image_path = \"/content/Flicker8k_Dataset\"\n",
        "dir_Flickr_text = \"/content/Flickr8k.token.txt\"\n",
        "\n",
        "jpgs = os.listdir(image_path)\n",
        "print(\"Total Images in Dataset = {}\".format(len(jpgs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check Image file name to Caption combination derived from the Flick8 dataset"
      ],
      "metadata": {
        "id": "0QXLDyt7Pxs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-xPRq-eMw8"
      },
      "source": [
        "file = open(dir_Flickr_text,'r')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "datatxt = []\n",
        "for line in text.split('\\n'):\n",
        "    col = line.split('\\t')\n",
        "    if len(col) == 1:\n",
        "        continue\n",
        "    w = col[0].split(\"#\")\n",
        "    datatxt.append(w + [col[1].lower()])\n",
        "\n",
        "data = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n",
        "data = data.reindex(columns =['index','filename','caption'])\n",
        "data = data[data['filename'] != '2258277193_586949ec62.jpg.1']\n",
        "\n",
        "\n",
        "uni_filenames = np.unique(data.filename.values)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print some images along with captions to check the Flickr Data that was imported"
      ],
      "metadata": {
        "id": "6s8A0_9APM9W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKujA2IkfBLL"
      },
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "npic = 5\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "\n",
        "count = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for jpgfnm in uni_filenames[10:15]:\n",
        "    filename = image_path + '/' + jpgfnm\n",
        "    captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n",
        "    image_load = load_img(filename, target_size=target_size)\n",
        "    \n",
        "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    count += 1\n",
        "    \n",
        "    ax = fig.add_subplot(npic,2,count)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.set_ylim(0,len(captions))\n",
        "    for i, caption in enumerate(captions):\n",
        "        ax.text(0,i,caption,fontsize=20)\n",
        "    count += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the words available i the captions by splitting the sentences"
      ],
      "metadata": {
        "id": "dZdcTv5LPc3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZfHfLEIgRra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45c1ed5-c335-45b8-9e6c-c6ec74856648"
      },
      "source": [
        "vocabulary = []\n",
        "for txt in data.caption.values:\n",
        "    vocabulary.extend(txt.split())\n",
        "print('Vocabulary Size: %d' % len(set(vocabulary)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 8918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crearting a list of Image file names and captions"
      ],
      "metadata": {
        "id": "JonxQQcSQ_6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIaD4ggfg0mc"
      },
      "source": [
        "img = data[\"filename\"].tolist()\n",
        "caption = data[\"caption\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkvs9_wr8gdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e425ce-3632-47e0-87db-dac0e053cf26"
      },
      "source": [
        "print(f\"len(img) : {len(img)}\")\n",
        "print(f\"len(caption) : {len(caption)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(img) : 40455\n",
            "len(caption) : 40455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1uklMU2P27-"
      },
      "source": [
        "\"\"\" Removing punctuations , Single characters , Numeric values from the word/Vocabulary , having them would make increase the probability of such words over others due to repetitive usage\n",
        "giving undesired results \"\"\"\n",
        "# To remove punctuations\n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(string.punctuation)\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "# To remove single characters\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "# To remove numeric values\n",
        "def remove_numeric(text):\n",
        "    text_no_numeric = \"\"\n",
        "    for word in text.split():\n",
        "        isalpha = word.isalpha()\n",
        "        if isalpha:\n",
        "            text_no_numeric += \" \" + word\n",
        "    return(text_no_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Cleaning up the original text without punctuations, single characters and numeric values \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lif_Si-nTJo9",
        "outputId": "2f9cc2c3-97c6-4a8e-f088-385c6177f955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Cleaning up the original text without punctuations, single characters and numeric values '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdEmhA-wPzpW"
      },
      "source": [
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    text = remove_numeric(text)\n",
        "    return(text)\n",
        "    \n",
        "for i, caption in enumerate(data.caption.values):\n",
        "    newcaption = text_clean(caption)\n",
        "    data[\"caption\"].iloc[i] = newcaption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXNQ0dxZQFoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091cf917-f5ad-4275-f856-a1b2f5937217"
      },
      "source": [
        "#building clean vocabulary\n",
        "clean_vocabulary = []\n",
        "for txt in data.caption.values:\n",
        "    clean_vocabulary.extend(txt.split())\n",
        "print('Clean Vocabulary Size: %d' % len(set(clean_vocabulary)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Vocabulary Size: 8357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUaf2Bc4BLBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970bc675-e260-4cbb-aa9e-49b53a16dbbc"
      },
      "source": [
        "# adding <start> and <end> tags to sentences to identify the start and end of the sentences while looping through words in sentence\n",
        "PATH = \"/content/Flicker8k_Dataset/\"\n",
        "all_captions = []\n",
        "\n",
        "for caption  in data[\"caption\"].astype(str):\n",
        "    caption = '<start> ' + caption+ ' <end>'\n",
        "    all_captions.append(caption)\n",
        "all_captions[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start>  child in pink dress is climbing up set of stairs in an entry way <end>',\n",
              " '<start>  girl going into wooden building <end>',\n",
              " '<start>  little girl climbing into wooden playhouse <end>',\n",
              " '<start>  little girl climbing the stairs to her playhouse <end>',\n",
              " '<start>  little girl in pink dress going into wooden cabin <end>',\n",
              " '<start>  black dog and spotted dog are fighting <end>',\n",
              " '<start>  black dog and dog playing with each other on the road <end>',\n",
              " '<start>  black dog and white dog with brown spots are staring at each other in the street <end>',\n",
              " '<start>  two dogs of different breeds looking at each other on the road <end>',\n",
              " '<start>  two dogs on pavement moving toward each other <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kABgi0fFvVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29653f90-ed98-4c62-8c7c-27c541bc9a57"
      },
      "source": [
        "#building path to read each image from the content downloaded\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in data[\"filename\"]:\n",
        "    full_image_path = PATH + annot\n",
        "\n",
        "    all_img_name_vector.append(full_image_path)\n",
        "all_img_name_vector[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
              " '/content/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
              " '/content/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
              " '/content/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
              " '/content/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
              " '/content/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
              " '/content/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
              " '/content/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
              " '/content/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
              " '/content/Flicker8k_Dataset/1001773457_577c3a7d70.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI-ZAolSGJv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b6600f-5fe1-4b73-fd66-756729e61da4"
      },
      "source": [
        "print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\n",
        "print(f\"len(all_captions) : {len(all_captions)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(all_img_name_vector) : 40455\n",
            "len(all_captions) : 40455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxxePX6TGTuF"
      },
      "source": [
        "def data_limiter(num,total_captions,all_img_name_vector):\n",
        "  train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n",
        "  train_captions = train_captions[:num]\n",
        "  img_name_vector = img_name_vector[:num]\n",
        "  return train_captions,img_name_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgNpDXZTHXT2"
      },
      "source": [
        "train_captions,img_name_vector = data_limiter(40000,all_captions,all_img_name_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW4pJ3TVuYQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c80178-c998-42a4-836f-1fafd535c5fc"
      },
      "source": [
        "print(f\"len(all_img_name_vector) : {len(img_name_vector)}\")\n",
        "print(f\"len(all_captions) : {len(train_captions)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(all_img_name_vector) : 40000\n",
            "len(all_captions) : 40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading an image with (224,224,3) for passing to Encoder Resnet50\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    return img, image_path"
      ],
      "metadata": {
        "id": "TiAWkrQeqNyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzJI3CC3cdRP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6a3990-7e98-4dfc-ef84-383d5f085814"
      },
      "source": [
        "image_model_ic = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
        "#Using Pretrained Resnet50 as encoder and taking Input size of (224,224,3)\n",
        "image_model = ResNet50(include_top=False,weights='imagenet',input_shape=(224, 224,3),pooling=\"avg\")\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-2].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n",
            "94781440/94765736 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_model_ic.summary()"
      ],
      "metadata": {
        "id": "wWYlrwMvQjzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e01e79-5b2a-410a-822e-0f3473912563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, None, None,   864         ['input_1[0][0]']                \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, None, None,   96         ['conv2d[0][0]']                 \n",
            " alization)                     32)                                                               \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, None, None,   0           ['batch_normalization[0][0]']    \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, None, None,   9216        ['activation[0][0]']             \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, None, None,   96         ['conv2d_1[0][0]']               \n",
            " rmalization)                   32)                                                               \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, None, None,   0           ['batch_normalization_1[0][0]']  \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, None, None,   18432       ['activation_1[0][0]']           \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, None, None,   192        ['conv2d_2[0][0]']               \n",
            " rmalization)                   64)                                                               \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, None, None,   0           ['batch_normalization_2[0][0]']  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, None, None,   0           ['activation_2[0][0]']           \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, None, None,   5120        ['max_pooling2d[0][0]']          \n",
            "                                80)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, None, None,   240        ['conv2d_3[0][0]']               \n",
            " rmalization)                   80)                                                               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, None, None,   0           ['batch_normalization_3[0][0]']  \n",
            "                                80)                                                               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, None, None,   138240      ['activation_3[0][0]']           \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, None, None,   576        ['conv2d_4[0][0]']               \n",
            " rmalization)                   192)                                                              \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, None, None,   0           ['batch_normalization_4[0][0]']  \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, None, None,   0          ['activation_4[0][0]']           \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, None, None,   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, None, None,   192        ['conv2d_8[0][0]']               \n",
            " rmalization)                   64)                                                               \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, None, None,   0           ['batch_normalization_8[0][0]']  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, None, None,   9216        ['max_pooling2d_1[0][0]']        \n",
            "                                48)                                                               \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, None, None,   55296       ['activation_8[0][0]']           \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, None, None,   144        ['conv2d_6[0][0]']               \n",
            " rmalization)                   48)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, None, None,   288        ['conv2d_9[0][0]']               \n",
            " rmalization)                   96)                                                               \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, None, None,   0           ['batch_normalization_6[0][0]']  \n",
            "                                48)                                                               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, None, None,   0           ['batch_normalization_9[0][0]']  \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, None, None,   0          ['max_pooling2d_1[0][0]']        \n",
            " ing2D)                         192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, None, None,   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, None, None,   76800       ['activation_6[0][0]']           \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, None, None,   82944       ['activation_9[0][0]']           \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, None, None,   6144        ['average_pooling2d[0][0]']      \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, None, None,   192        ['conv2d_5[0][0]']               \n",
            " rmalization)                   64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, None, None,   192        ['conv2d_7[0][0]']               \n",
            " rmalization)                   64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, None, None,   288        ['conv2d_10[0][0]']              \n",
            " ormalization)                  96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, None, None,   96         ['conv2d_11[0][0]']              \n",
            " ormalization)                  32)                                                               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, None, None,   0           ['batch_normalization_5[0][0]']  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, None, None,   0           ['batch_normalization_7[0][0]']  \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, None, None,   0           ['batch_normalization_10[0][0]'] \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, None, None,   0           ['batch_normalization_11[0][0]'] \n",
            "                                32)                                                               \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)           (None, None, None,   0           ['activation_5[0][0]',           \n",
            "                                256)                              'activation_7[0][0]',           \n",
            "                                                                  'activation_10[0][0]',          \n",
            "                                                                  'activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, None, None,   16384       ['mixed0[0][0]']                 \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, None, None,   192        ['conv2d_15[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, None, None,   0           ['batch_normalization_15[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, None, None,   12288       ['mixed0[0][0]']                 \n",
            "                                48)                                                               \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, None, None,   55296       ['activation_15[0][0]']          \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, None, None,   144        ['conv2d_13[0][0]']              \n",
            " ormalization)                  48)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, None, None,   288        ['conv2d_16[0][0]']              \n",
            " ormalization)                  96)                                                               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, None, None,   0           ['batch_normalization_13[0][0]'] \n",
            "                                48)                                                               \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, None, None,   0           ['batch_normalization_16[0][0]'] \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, None, None,   0          ['mixed0[0][0]']                 \n",
            " oling2D)                       256)                                                              \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, None, None,   16384       ['mixed0[0][0]']                 \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, None, None,   76800       ['activation_13[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, None, None,   82944       ['activation_16[0][0]']          \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, None, None,   16384       ['average_pooling2d_1[0][0]']    \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, None, None,   192        ['conv2d_12[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, None, None,   192        ['conv2d_14[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, None, None,   288        ['conv2d_17[0][0]']              \n",
            " ormalization)                  96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, None, None,   192        ['conv2d_18[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, None, None,   0           ['batch_normalization_12[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, None, None,   0           ['batch_normalization_14[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, None, None,   0           ['batch_normalization_17[0][0]'] \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, None, None,   0           ['batch_normalization_18[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)           (None, None, None,   0           ['activation_12[0][0]',          \n",
            "                                288)                              'activation_14[0][0]',          \n",
            "                                                                  'activation_17[0][0]',          \n",
            "                                                                  'activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, None, None,   18432       ['mixed1[0][0]']                 \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, None, None,   192        ['conv2d_22[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, None, None,   0           ['batch_normalization_22[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, None, None,   13824       ['mixed1[0][0]']                 \n",
            "                                48)                                                               \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, None, None,   55296       ['activation_22[0][0]']          \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, None, None,   144        ['conv2d_20[0][0]']              \n",
            " ormalization)                  48)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, None, None,   288        ['conv2d_23[0][0]']              \n",
            " ormalization)                  96)                                                               \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, None, None,   0           ['batch_normalization_20[0][0]'] \n",
            "                                48)                                                               \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, None, None,   0           ['batch_normalization_23[0][0]'] \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, None, None,   0          ['mixed1[0][0]']                 \n",
            " oling2D)                       288)                                                              \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, None, None,   18432       ['mixed1[0][0]']                 \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, None, None,   76800       ['activation_20[0][0]']          \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, None, None,   82944       ['activation_23[0][0]']          \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, None, None,   18432       ['average_pooling2d_2[0][0]']    \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, None, None,   192        ['conv2d_19[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, None, None,   192        ['conv2d_21[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, None, None,   288        ['conv2d_24[0][0]']              \n",
            " ormalization)                  96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, None, None,   192        ['conv2d_25[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, None, None,   0           ['batch_normalization_19[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, None, None,   0           ['batch_normalization_21[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, None, None,   0           ['batch_normalization_24[0][0]'] \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, None, None,   0           ['batch_normalization_25[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)           (None, None, None,   0           ['activation_19[0][0]',          \n",
            "                                288)                              'activation_21[0][0]',          \n",
            "                                                                  'activation_24[0][0]',          \n",
            "                                                                  'activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, None, None,   18432       ['mixed2[0][0]']                 \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, None, None,   192        ['conv2d_27[0][0]']              \n",
            " ormalization)                  64)                                                               \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, None, None,   0           ['batch_normalization_27[0][0]'] \n",
            "                                64)                                                               \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, None, None,   55296       ['activation_27[0][0]']          \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, None, None,   288        ['conv2d_28[0][0]']              \n",
            " ormalization)                  96)                                                               \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, None, None,   0           ['batch_normalization_28[0][0]'] \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, None, None,   995328      ['mixed2[0][0]']                 \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, None, None,   82944       ['activation_28[0][0]']          \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, None, None,   1152       ['conv2d_26[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, None, None,   288        ['conv2d_29[0][0]']              \n",
            " ormalization)                  96)                                                               \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, None, None,   0           ['batch_normalization_26[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, None, None,   0           ['batch_normalization_29[0][0]'] \n",
            "                                96)                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, None, None,   0          ['mixed2[0][0]']                 \n",
            "                                288)                                                              \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)           (None, None, None,   0           ['activation_26[0][0]',          \n",
            "                                768)                              'activation_29[0][0]',          \n",
            "                                                                  'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, None, None,   98304       ['mixed3[0][0]']                 \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, None, None,   384        ['conv2d_34[0][0]']              \n",
            " ormalization)                  128)                                                              \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, None, None,   0           ['batch_normalization_34[0][0]'] \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, None, None,   114688      ['activation_34[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, None, None,   384        ['conv2d_35[0][0]']              \n",
            " ormalization)                  128)                                                              \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, None, None,   0           ['batch_normalization_35[0][0]'] \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, None, None,   98304       ['mixed3[0][0]']                 \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, None, None,   114688      ['activation_35[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, None, None,   384        ['conv2d_31[0][0]']              \n",
            " ormalization)                  128)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, None, None,   384        ['conv2d_36[0][0]']              \n",
            " ormalization)                  128)                                                              \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, None, None,   0           ['batch_normalization_31[0][0]'] \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, None, None,   0           ['batch_normalization_36[0][0]'] \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, None, None,   114688      ['activation_31[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, None, None,   114688      ['activation_36[0][0]']          \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, None, None,   384        ['conv2d_32[0][0]']              \n",
            " ormalization)                  128)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, None, None,   384        ['conv2d_37[0][0]']              \n",
            " ormalization)                  128)                                                              \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, None, None,   0           ['batch_normalization_32[0][0]'] \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, None, None,   0           ['batch_normalization_37[0][0]'] \n",
            "                                128)                                                              \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, None, None,   0          ['mixed3[0][0]']                 \n",
            " oling2D)                       768)                                                              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, None, None,   147456      ['mixed3[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, None, None,   172032      ['activation_32[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, None, None,   172032      ['activation_37[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, None, None,   147456      ['average_pooling2d_3[0][0]']    \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, None, None,   576        ['conv2d_30[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, None, None,   576        ['conv2d_33[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, None, None,   576        ['conv2d_38[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, None, None,   576        ['conv2d_39[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, None, None,   0           ['batch_normalization_30[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, None, None,   0           ['batch_normalization_33[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, None, None,   0           ['batch_normalization_38[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, None, None,   0           ['batch_normalization_39[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)           (None, None, None,   0           ['activation_30[0][0]',          \n",
            "                                768)                              'activation_33[0][0]',          \n",
            "                                                                  'activation_38[0][0]',          \n",
            "                                                                  'activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, None, None,   122880      ['mixed4[0][0]']                 \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, None, None,   480        ['conv2d_44[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, None, None,   0           ['batch_normalization_44[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, None, None,   179200      ['activation_44[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, None, None,   480        ['conv2d_45[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, None, None,   0           ['batch_normalization_45[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, None, None,   122880      ['mixed4[0][0]']                 \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, None, None,   179200      ['activation_45[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, None, None,   480        ['conv2d_41[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, None, None,   480        ['conv2d_46[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, None, None,   0           ['batch_normalization_41[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, None, None,   0           ['batch_normalization_46[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, None, None,   179200      ['activation_41[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, None, None,   179200      ['activation_46[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, None, None,   480        ['conv2d_42[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, None, None,   480        ['conv2d_47[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, None, None,   0           ['batch_normalization_42[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, None, None,   0           ['batch_normalization_47[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " average_pooling2d_4 (AveragePo  (None, None, None,   0          ['mixed4[0][0]']                 \n",
            " oling2D)                       768)                                                              \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, None, None,   147456      ['mixed4[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, None, None,   215040      ['activation_42[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, None, None,   215040      ['activation_47[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, None, None,   147456      ['average_pooling2d_4[0][0]']    \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, None, None,   576        ['conv2d_40[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, None, None,   576        ['conv2d_43[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, None, None,   576        ['conv2d_48[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, None, None,   576        ['conv2d_49[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, None, None,   0           ['batch_normalization_40[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, None, None,   0           ['batch_normalization_43[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, None, None,   0           ['batch_normalization_48[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, None, None,   0           ['batch_normalization_49[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)           (None, None, None,   0           ['activation_40[0][0]',          \n",
            "                                768)                              'activation_43[0][0]',          \n",
            "                                                                  'activation_48[0][0]',          \n",
            "                                                                  'activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, None, None,   122880      ['mixed5[0][0]']                 \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, None, None,   480        ['conv2d_54[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, None, None,   0           ['batch_normalization_54[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, None, None,   179200      ['activation_54[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, None, None,   480        ['conv2d_55[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, None, None,   0           ['batch_normalization_55[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, None, None,   122880      ['mixed5[0][0]']                 \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, None, None,   179200      ['activation_55[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, None, None,   480        ['conv2d_51[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, None, None,   480        ['conv2d_56[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, None, None,   0           ['batch_normalization_51[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, None, None,   0           ['batch_normalization_56[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, None, None,   179200      ['activation_51[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, None, None,   179200      ['activation_56[0][0]']          \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, None, None,   480        ['conv2d_52[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, None, None,   480        ['conv2d_57[0][0]']              \n",
            " ormalization)                  160)                                                              \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, None, None,   0           ['batch_normalization_52[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, None, None,   0           ['batch_normalization_57[0][0]'] \n",
            "                                160)                                                              \n",
            "                                                                                                  \n",
            " average_pooling2d_5 (AveragePo  (None, None, None,   0          ['mixed5[0][0]']                 \n",
            " oling2D)                       768)                                                              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, None, None,   147456      ['mixed5[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, None, None,   215040      ['activation_52[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, None, None,   215040      ['activation_57[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, None, None,   147456      ['average_pooling2d_5[0][0]']    \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, None, None,   576        ['conv2d_50[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, None, None,   576        ['conv2d_53[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, None, None,   576        ['conv2d_58[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, None, None,   576        ['conv2d_59[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, None, None,   0           ['batch_normalization_50[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, None, None,   0           ['batch_normalization_53[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, None, None,   0           ['batch_normalization_58[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, None, None,   0           ['batch_normalization_59[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)           (None, None, None,   0           ['activation_50[0][0]',          \n",
            "                                768)                              'activation_53[0][0]',          \n",
            "                                                                  'activation_58[0][0]',          \n",
            "                                                                  'activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, None, None,   147456      ['mixed6[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, None, None,   576        ['conv2d_64[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, None, None,   0           ['batch_normalization_64[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, None, None,   258048      ['activation_64[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, None, None,   576        ['conv2d_65[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, None, None,   0           ['batch_normalization_65[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, None, None,   147456      ['mixed6[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, None, None,   258048      ['activation_65[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, None, None,   576        ['conv2d_61[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, None, None,   576        ['conv2d_66[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, None, None,   0           ['batch_normalization_61[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, None, None,   0           ['batch_normalization_66[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, None, None,   258048      ['activation_61[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, None, None,   258048      ['activation_66[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, None, None,   576        ['conv2d_62[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, None, None,   576        ['conv2d_67[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, None, None,   0           ['batch_normalization_62[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, None, None,   0           ['batch_normalization_67[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " average_pooling2d_6 (AveragePo  (None, None, None,   0          ['mixed6[0][0]']                 \n",
            " oling2D)                       768)                                                              \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, None, None,   147456      ['mixed6[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, None, None,   258048      ['activation_62[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, None, None,   258048      ['activation_67[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, None, None,   147456      ['average_pooling2d_6[0][0]']    \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, None, None,   576        ['conv2d_60[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, None, None,   576        ['conv2d_63[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, None, None,   576        ['conv2d_68[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, None, None,   576        ['conv2d_69[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, None, None,   0           ['batch_normalization_60[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, None, None,   0           ['batch_normalization_63[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, None, None,   0           ['batch_normalization_68[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, None, None,   0           ['batch_normalization_69[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)           (None, None, None,   0           ['activation_60[0][0]',          \n",
            "                                768)                              'activation_63[0][0]',          \n",
            "                                                                  'activation_68[0][0]',          \n",
            "                                                                  'activation_69[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, None, None,   147456      ['mixed7[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, None, None,   576        ['conv2d_72[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, None, None,   0           ['batch_normalization_72[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, None, None,   258048      ['activation_72[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, None, None,   576        ['conv2d_73[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, None, None,   0           ['batch_normalization_73[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, None, None,   147456      ['mixed7[0][0]']                 \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, None, None,   258048      ['activation_73[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, None, None,   576        ['conv2d_70[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, None, None,   576        ['conv2d_74[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, None, None,   0           ['batch_normalization_70[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, None, None,   0           ['batch_normalization_74[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, None, None,   552960      ['activation_70[0][0]']          \n",
            "                                320)                                                              \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, None, None,   331776      ['activation_74[0][0]']          \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, None, None,   960        ['conv2d_71[0][0]']              \n",
            " ormalization)                  320)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, None, None,   576        ['conv2d_75[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, None, None,   0           ['batch_normalization_71[0][0]'] \n",
            "                                320)                                                              \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, None, None,   0           ['batch_normalization_75[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, None, None,   0          ['mixed7[0][0]']                 \n",
            "                                768)                                                              \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)           (None, None, None,   0           ['activation_71[0][0]',          \n",
            "                                1280)                             'activation_75[0][0]',          \n",
            "                                                                  'max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, None, None,   573440      ['mixed8[0][0]']                 \n",
            "                                448)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, None, None,   1344       ['conv2d_80[0][0]']              \n",
            " ormalization)                  448)                                                              \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, None, None,   0           ['batch_normalization_80[0][0]'] \n",
            "                                448)                                                              \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, None, None,   491520      ['mixed8[0][0]']                 \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, None, None,   1548288     ['activation_80[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, None, None,   1152       ['conv2d_77[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_81 (BatchN  (None, None, None,   1152       ['conv2d_81[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, None, None,   0           ['batch_normalization_77[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, None, None,   0           ['batch_normalization_81[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, None, None,   442368      ['activation_77[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, None, None,   442368      ['activation_77[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, None, None,   442368      ['activation_81[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, None, None,   442368      ['activation_81[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " average_pooling2d_7 (AveragePo  (None, None, None,   0          ['mixed8[0][0]']                 \n",
            " oling2D)                       1280)                                                             \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, None, None,   409600      ['mixed8[0][0]']                 \n",
            "                                320)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, None, None,   1152       ['conv2d_78[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, None, None,   1152       ['conv2d_79[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_82 (BatchN  (None, None, None,   1152       ['conv2d_82[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_83 (BatchN  (None, None, None,   1152       ['conv2d_83[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, None, None,   245760      ['average_pooling2d_7[0][0]']    \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, None, None,   960        ['conv2d_76[0][0]']              \n",
            " ormalization)                  320)                                                              \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, None, None,   0           ['batch_normalization_78[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, None, None,   0           ['batch_normalization_79[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_82 (Activation)     (None, None, None,   0           ['batch_normalization_82[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_83 (Activation)     (None, None, None,   0           ['batch_normalization_83[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_84 (BatchN  (None, None, None,   576        ['conv2d_84[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, None, None,   0           ['batch_normalization_76[0][0]'] \n",
            "                                320)                                                              \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)         (None, None, None,   0           ['activation_78[0][0]',          \n",
            "                                768)                              'activation_79[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, None, None,   0           ['activation_82[0][0]',          \n",
            "                                768)                              'activation_83[0][0]']          \n",
            "                                                                                                  \n",
            " activation_84 (Activation)     (None, None, None,   0           ['batch_normalization_84[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)           (None, None, None,   0           ['activation_76[0][0]',          \n",
            "                                2048)                             'mixed9_0[0][0]',               \n",
            "                                                                  'concatenate[0][0]',            \n",
            "                                                                  'activation_84[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, None, None,   917504      ['mixed9[0][0]']                 \n",
            "                                448)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, None, None,   1344       ['conv2d_89[0][0]']              \n",
            " ormalization)                  448)                                                              \n",
            "                                                                                                  \n",
            " activation_89 (Activation)     (None, None, None,   0           ['batch_normalization_89[0][0]'] \n",
            "                                448)                                                              \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, None, None,   786432      ['mixed9[0][0]']                 \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)             (None, None, None,   1548288     ['activation_89[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, None, None,   1152       ['conv2d_86[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, None, None,   1152       ['conv2d_90[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " activation_86 (Activation)     (None, None, None,   0           ['batch_normalization_86[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_90 (Activation)     (None, None, None,   0           ['batch_normalization_90[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, None, None,   442368      ['activation_86[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, None, None,   442368      ['activation_86[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, None, None,   442368      ['activation_90[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)             (None, None, None,   442368      ['activation_90[0][0]']          \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " average_pooling2d_8 (AveragePo  (None, None, None,   0          ['mixed9[0][0]']                 \n",
            " oling2D)                       2048)                                                             \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, None, None,   655360      ['mixed9[0][0]']                 \n",
            "                                320)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, None, None,   1152       ['conv2d_87[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, None, None,   1152       ['conv2d_88[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, None, None,   1152       ['conv2d_91[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_92 (BatchN  (None, None, None,   1152       ['conv2d_92[0][0]']              \n",
            " ormalization)                  384)                                                              \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)             (None, None, None,   393216      ['average_pooling2d_8[0][0]']    \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, None, None,   960        ['conv2d_85[0][0]']              \n",
            " ormalization)                  320)                                                              \n",
            "                                                                                                  \n",
            " activation_87 (Activation)     (None, None, None,   0           ['batch_normalization_87[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_88 (Activation)     (None, None, None,   0           ['batch_normalization_88[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_91 (Activation)     (None, None, None,   0           ['batch_normalization_91[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " activation_92 (Activation)     (None, None, None,   0           ['batch_normalization_92[0][0]'] \n",
            "                                384)                                                              \n",
            "                                                                                                  \n",
            " batch_normalization_93 (BatchN  (None, None, None,   576        ['conv2d_93[0][0]']              \n",
            " ormalization)                  192)                                                              \n",
            "                                                                                                  \n",
            " activation_85 (Activation)     (None, None, None,   0           ['batch_normalization_85[0][0]'] \n",
            "                                320)                                                              \n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)         (None, None, None,   0           ['activation_87[0][0]',          \n",
            "                                768)                              'activation_88[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, None, None,   0           ['activation_91[0][0]',          \n",
            "                                768)                              'activation_92[0][0]']          \n",
            "                                                                                                  \n",
            " activation_93 (Activation)     (None, None, None,   0           ['batch_normalization_93[0][0]'] \n",
            "                                192)                                                              \n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)          (None, None, None,   0           ['activation_85[0][0]',          \n",
            "                                2048)                             'mixed9_1[0][0]',               \n",
            "                                                                  'concatenate_1[0][0]',          \n",
            "                                                                  'activation_93[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssogBxRjKAlR",
        "outputId": "40784b20-6412-4dfb-e448-087b49ecff3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tGPHHZlchUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f318f137-d957-4f81-c46a-3edc4f4456e3"
      },
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Batch size 16\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "#Building Image feature vector using Resnet50 Encoder\n",
        "for img, path in tqdm(image_dataset):\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  # converting image from Resnet (16,7,7,2048) to (16,49,2048)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:22<00:00,  6.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temp code\n",
        "for img1, path1 in tqdm(image_dataset):\n",
        "  batch_features1 = image_features_extract_model(img1)\n",
        "  print(\"shape\",batch_features1.shape)\n",
        "  print(\"batch_features.shape[0]\",batch_features1.shape[0])\n",
        "  print(\"batch_features.shape[1]\",batch_features1.shape[1])\n",
        "  print(\"batch_features.shape[2]\",batch_features1.shape[2])\n",
        "  print(\"batch_features.shape[3]\",batch_features1.shape[3])\n",
        "  batch_features2 = tf.reshape(batch_features1,\n",
        "                              (batch_features1.shape[0], -1, batch_features1.shape[3]))\n",
        "  print(\"shape\",batch_features2.shape)\n",
        "  print(\"batch_features2.shape[0]\",batch_features2.shape[0])\n",
        "  print(\"batch_features2.shape[1]\",batch_features2.shape[1])\n",
        "  print(\"batch_features2.shape[2]\",batch_features2.shape[2])\n",
        "  #print(\"batch_features2.shape[3]\",batch_features2.shape[3])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL4xNfQi-9sY",
        "outputId": "d50e19fc-53cd-4adc-f41f-761a1f76beb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/506 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape (16, 7, 7, 2048)\n",
            "batch_features.shape[0] 16\n",
            "batch_features.shape[1] 7\n",
            "batch_features.shape[2] 7\n",
            "batch_features.shape[3] 2048\n",
            "shape (16, 49, 2048)\n",
            "batch_features2.shape[0] 16\n",
            "batch_features2.shape[1] 49\n",
            "batch_features2.shape[2] 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMLfDYAfdH93"
      },
      "source": [
        "### Preprocess and tokenize the captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCafUvDr8D2r"
      },
      "source": [
        "top_k = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWXKqpL5dBss"
      },
      "source": [
        "#Building a Word embedding for top 5000 words in the captions\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq14GqxndRn6"
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B85IjQRQdWCV"
      },
      "source": [
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qGqYT73dmmb"
      },
      "source": [
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BMJeoCfdqsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf53f14-d6a8-4fc7-fa71-1d0839a76b1a"
      },
      "source": [
        "cap_vector.shape # Maximum number of words in a sentence is 33 (in the captions dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVq2KakYdVN9"
      },
      "source": [
        "### Split the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OKymw5Hd037"
      },
      "source": [
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9J7JXejd8eD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38e7ab5-94a5-46de-f3c2-6c090167b415"
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32000, 32000, 8000, 8000)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUGdUhcfeAq8"
      },
      "source": [
        "### Create a tf.data dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Temp code\n",
        "print(cap_train.shape)\n",
        "print (type(cap_train))\n",
        "cap_train[0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ci-7I6Uq9ZG",
        "outputId": "c71c0e25-04e7-4660-8a34-56cc955901bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32000, 33)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,  11, 922,  29, 171,  20,  29, 159,   6,  84, 110,   3,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU_j7m48eGSn"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "num_steps = len(img_name_train) // BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anmVgUjzeZOw"
      },
      "source": [
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlChoPCGeab0"
      },
      "source": [
        "# Check what is this ?\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "   angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "   return pos * angle_rates"
      ],
      "metadata": {
        "id": "WE4Xs5StjgN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNARmUUYMbyi"
      },
      "source": [
        "# Position encoding for 1d vectors ( Word embedding)\n",
        "def positional_encoding_1d(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAjxo7YlMg6p"
      },
      "source": [
        "# Position encoding for 2d vectors (image)\n",
        "#Input (Batch Size, H*W , Dense layer from Encoder)\n",
        "def positional_encoding_2d(row,col,d_model):\n",
        "  assert d_model % 2 == 0\n",
        "  # first d_model/2 encode row embedding and second d_model/2 encode column embedding\n",
        "  row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n",
        "  col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n",
        "  angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
        "  angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n",
        "  #apply sin and cos to odd and even indices resp.\n",
        "  angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
        "  angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
        "  angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
        "  angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
        "  pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPtqOKOfiNM6"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od9Ltf9qkJiu"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jPObHU0k2SC"
      },
      "source": [
        "# This block build weights for Q - Query , K - Key , V - Value for the 8 heads\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  #adding -Inf where mask is 1 s.t. value get ignored in softmax\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iduottdan-sp"
      },
      "source": [
        "\"\"\" This class build Multihead attention layer with number of heads passed ( 8 as per paper) and builds\n",
        "Weigth for q - Query , k - Key , v - Value  . Weights matrix will be as many as number of heads \"\"\"\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    #\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask=None):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF7Zq49KqvDw"
      },
      "source": [
        "# fucntion converts the Feedforward layer vector to Model size ( 512 in this case)\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks9Oo02jsN3D"
      },
      "source": [
        "\"\"\" Within this block the Multihead attention layer is built to get weights for q,k,v \n",
        " The output of MHA is passed to FFN and normalization\"\"\"\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask=None):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3ZXI1RVw2Rf"
      },
      "source": [
        "\"\"\" Decoder layer will build two MHA \n",
        "1. MHA1 = Word Embedding + Position Encoding\n",
        "2. MHA2 = MHA1 + MHA_ecoded( image)\n",
        "Then it creates a Feedforward layer with d_model channels ( 512 here)\n",
        "Returns  batch normalized output with MHA1 and MHA2 Attention weights\"\"\"\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask=None, padding_mask=None):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    # using look ahead mask so that during self attention current query dont consider future token\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    # use padding mask to avoid padded values of both enc_output and dec_input\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_layer = 4\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "row_size = 7\n",
        "col_size = 7\n",
        "target_vocab_size = top_k + 1 # top_k = 5000\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "-lj0LYM8ucSW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6wYw3lpyYWL"
      },
      "source": [
        "\"\"\"Encoder to the following\n",
        "Input - Takes Image Feature vector from Resnet50 \n",
        "Adds position encoding to Image vector , builds an attention vector for the image\n",
        "Output - updated Image vector with attention calculated  \n",
        " \"\"\"\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "               row_size,col_size,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n",
        "    self.pos_encoding = positional_encoding_2d(row_size,col_size, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask=None):\n",
        "    # shape(x) = (batch_size,seq_len(H*W),features)\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len(H*W), d_model)\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask) # Encode layer is execture 4 times ?\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOQ_DNET2kgO"
      },
      "source": [
        "\"\"\" Decoder take the Word emdedding as input adds the Postion encoding to it and build a new embedding\n",
        "New embedding is passed to the Decoder layer . Two key calcualtions happen in DecoderLayer\n",
        "1. Calculation of Attention weights for New Word embedding \n",
        "2. Calculation of attention weights for Word Embedding + Attention Image ( out of Encoded layer)\n",
        "This is passed to a feed forward output is derived\"\"\"\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask=None, padding_mask=None):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))# WHY ?\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv5-Vrse4_uK"
      },
      "source": [
        "\"\"\" This class build Encoder , Decoder and Encoder+ decoder inputs and passes build a layer size Vocab\n",
        "The out of this class would a predicted \"\"\"\n",
        "class Transformer(tf.keras.Model):\n",
        "#_int_(num_layers = 4, d_model=512 , num_heads = 8, dff=2048,row_size=7,col_soze = 7, target_vocab_size = 5000+1 \n",
        "#,max_pos_encoding = vocab_size,rate=0.1 )\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff,row_size,col_size, \n",
        "               target_vocab_size,max_pos_encoding, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,row_size,col_size, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size,max_pos_encoding, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "# inp = Image tensor , tar = Word embedding , training = True \n",
        "  def call(self, inp, tar, training,look_ahead_mask=None, dec_padding_mask=None,enc_padding_mask=None):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz2eNi5G7RkQ"
      },
      "source": [
        "num_layer = 4\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "row_size = 7\n",
        "col_size = 7\n",
        "target_vocab_size = top_k + 1 # top_k = 5000\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer = Transformer(num_layer,d_model,num_heads,dff,row_size,col_size,target_vocab_size,max_pos_encoding=target_vocab_size,rate=dropout_rate)"
      ],
      "metadata": {
        "id": "9dOmAhmbtTRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (target_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulyrv9MndKYX",
        "outputId": "31a2f620-de13-4add-8189-38dcd77d87a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(target_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwEe7FcPdRbv",
        "outputId": "15354322-9f08-4fc7-c9ed-e2662d8887d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    1,    2, ..., 4998, 4999, 5000])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temp code\n",
        "angle_check = get_angles(np.arange(target_vocab_size)[:, np.newaxis],\n",
        "                          np.arange(512)[np.newaxis, :],\n",
        "                          512)\n",
        "print (angle_check.shape)\n",
        "print(type(angle_check))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2i2XhMHdeRQ",
        "outputId": "d529f5c2-8a7f-4687-de25-07c8bc2efe2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5001, 512)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temp code\n",
        "# apply sin to even indices in the array; 2i\n",
        "angle_check[:, 0::2] = np.sin(angle_check[:, 0::2])\n",
        "print(angle_check.shape)\n",
        "# apply cos to odd indices in the array; 2i+1\n",
        "angle_check[:, 1::2] = np.cos(angle_check[:, 1::2])\n",
        "pos_encoding_check = angle_check[np.newaxis, ...]\n",
        "print(pos_encoding_check.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_1mrj3LeUwo",
        "outputId": "5a208a68-692b-4d03-8dba-074872a61a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5001, 512)\n",
            "(1, 5001, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Temp code\n",
        "  row_pos1 = np.repeat(np.arange(8),8)[:,np.newaxis]\n",
        "  print(\"row_pos1\", row_pos1.shape)\n",
        "  col_pos1 = np.repeat(np.expand_dims(np.arange(8),0),8,axis=0).reshape(-1,1)\n",
        "  print(\"row_pos1\", col_pos1.shape)\n",
        "  angle_rads_row1 = get_angles(row_pos1,np.arange(512//2)[np.newaxis,:],d_model//2)\n",
        "  print(\"angle_rads_row1\", angle_rads_row1.shape)\n",
        "  angle_rads_col1 = get_angles(col_pos1,np.arange(512//2)[np.newaxis,:],d_model//2)\n",
        "  print(\"angle_rads_col1\", angle_rads_col1.shape)\n",
        "  #apply sin and cos to odd and even indices resp.\n",
        "  angle_rads_row1[:, 0::2] = np.sin(angle_rads_row1[:, 0::2])\n",
        "  angle_rads_row1[:, 1::2] = np.cos(angle_rads_row1[:, 1::2])\n",
        "  print(\"angle_rads_row1-1\", angle_rads_row1.shape)\n",
        "  print(\"angle_rads_col1-1\", angle_rads_col1.shape)\n",
        "  angle_rads_col1[:, 0::2] = np.sin(angle_rads_col1[:, 0::2])\n",
        "  angle_rads_col1[:, 1::2] = np.cos(angle_rads_col1[:, 1::2])\n",
        "  print(\"angle_rads_row1-2\", angle_rads_row1.shape)\n",
        "  print(\"angle_rads_col1-2\", angle_rads_col1.shape)\n",
        "  pos_encoding1 = np.concatenate([angle_rads_row1,angle_rads_col1],axis=1)[np.newaxis, ...]\n",
        "  print(\"pos_encoding1\", pos_encoding1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jry4O8qNgAkk",
        "outputId": "06c07917-d063-4089-b9dc-326497a47e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row_pos1 (64, 1)\n",
            "row_pos1 (64, 1)\n",
            "angle_rads_row1 (64, 256)\n",
            "angle_rads_col1 (64, 256)\n",
            "angle_rads_row1-1 (64, 256)\n",
            "angle_rads_col1-1 (64, 256)\n",
            "angle_rads_row1-2 (64, 256)\n",
            "angle_rads_col1-2 (64, 256)\n",
            "pos_encoding1 (1, 64, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsjF1eoTT3eQ"
      },
      "source": [
        "# What is this?\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SSH-4bOTJff"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JHB2oUSUmX5"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkkzdkRvUh4A"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqXuReTiU-fq"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM9u-U3BVEW0"
      },
      "source": [
        "transformer = Transformer(num_layer,d_model,num_heads,dff,row_size,col_size,target_vocab_size,max_pos_encoding=target_vocab_size,rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3LHMBwxVohZ"
      },
      "source": [
        "def create_masks_decoder(tar):\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  return combined_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW12LgraY3sP"
      },
      "source": [
        "@tf.function\n",
        "#Image tensor and correspoding caption is passed to train_step\n",
        "def train_step(img_tensor, tar):\n",
        "  tar_inp = tar[:, :-1] ## all word embedding except the <end>\n",
        "  tar_real = tar[:, 1:] ## all word embedding except the <start>\n",
        "  \n",
        "  dec_mask = create_masks_decoder(tar_inp)# Setting all embeddding for future words to 0 and taking only previous words\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(img_tensor, tar_inp, \n",
        "                                 True,  \n",
        "                                 dec_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #temp code\n",
        " for (batch1, (img_tensor1, tar1)) in enumerate(dataset):\n",
        "     print(\"img_tensor1\", img_tensor1.shape)\n",
        "     print(\"tar1\" , tar1.shape)\n",
        "     print( tar1[:, :-1].shape)\n",
        "     print( tar1[:, 1:].shape)\n",
        "     dec_mask1 = create_masks_decoder(tar1[:, :-1])\n",
        "     print(dec_mask1.shape)\n",
        "     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZECvPmFD-VnS",
        "outputId": "d14939b6-53f7-4668-a40b-d491dbcd5cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_tensor1 (64, 49, 2048)\n",
            "tar1 (64, 33)\n",
            "(64, 32)\n",
            "(64, 32)\n",
            "(64, 1, 32, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC41CqxIaadN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfd4f9b-bc55-415d-aba6-2f7298395772"
      },
      "source": [
        "#Epochs = 70 , Data set returns Image and All captions for images which are passed to train_step function\n",
        "for epoch in range(70):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  for (batch, (img_tensor, tar)) in enumerate(dataset):\n",
        "    train_step(img_tensor, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 8.5716 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.1275 Accuracy 0.0205\n",
            "Epoch 1 Batch 100 Loss 7.7180 Accuracy 0.0258\n",
            "Epoch 1 Batch 150 Loss 7.4166 Accuracy 0.0279\n",
            "Epoch 1 Batch 200 Loss 7.1237 Accuracy 0.0309\n",
            "Epoch 1 Batch 250 Loss 6.8455 Accuracy 0.0347\n",
            "Epoch 1 Batch 300 Loss 6.5961 Accuracy 0.0386\n",
            "Epoch 1 Batch 350 Loss 6.3795 Accuracy 0.0422\n",
            "Epoch 1 Batch 400 Loss 6.1892 Accuracy 0.0456\n",
            "Epoch 1 Batch 450 Loss 6.0200 Accuracy 0.0487\n",
            "Epoch 1 Loss 5.8756 Accuracy 0.0514\n",
            "Time taken for 1 epoch: 87.22046756744385 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.4034 Accuracy 0.0747\n",
            "Epoch 2 Batch 50 Loss 4.4241 Accuracy 0.0800\n",
            "Epoch 2 Batch 100 Loss 4.3601 Accuracy 0.0818\n",
            "Epoch 2 Batch 150 Loss 4.3138 Accuracy 0.0824\n",
            "Epoch 2 Batch 200 Loss 4.2662 Accuracy 0.0834\n",
            "Epoch 2 Batch 250 Loss 4.2219 Accuracy 0.0842\n",
            "Epoch 2 Batch 300 Loss 4.1844 Accuracy 0.0851\n",
            "Epoch 2 Batch 350 Loss 4.1485 Accuracy 0.0858\n",
            "Epoch 2 Batch 400 Loss 4.1106 Accuracy 0.0866\n",
            "Epoch 2 Batch 450 Loss 4.0735 Accuracy 0.0876\n",
            "Epoch 2 Loss 4.0452 Accuracy 0.0881\n",
            "Time taken for 1 epoch: 69.1038920879364 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.7492 Accuracy 0.0928\n",
            "Epoch 3 Batch 50 Loss 3.7158 Accuracy 0.0959\n",
            "Epoch 3 Batch 100 Loss 3.6915 Accuracy 0.0971\n",
            "Epoch 3 Batch 150 Loss 3.6700 Accuracy 0.0976\n",
            "Epoch 3 Batch 200 Loss 3.6528 Accuracy 0.0980\n",
            "Epoch 3 Batch 250 Loss 3.6327 Accuracy 0.0984\n",
            "Epoch 3 Batch 300 Loss 3.6153 Accuracy 0.0990\n",
            "Epoch 3 Batch 350 Loss 3.5993 Accuracy 0.0994\n",
            "Epoch 3 Batch 400 Loss 3.5809 Accuracy 0.0997\n",
            "Epoch 3 Batch 450 Loss 3.5621 Accuracy 0.1001\n",
            "Epoch 3 Loss 3.5513 Accuracy 0.1004\n",
            "Time taken for 1 epoch: 69.20977115631104 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.3600 Accuracy 0.1035\n",
            "Epoch 4 Batch 50 Loss 3.3930 Accuracy 0.1045\n",
            "Epoch 4 Batch 100 Loss 3.3751 Accuracy 0.1058\n",
            "Epoch 4 Batch 150 Loss 3.3694 Accuracy 0.1053\n",
            "Epoch 4 Batch 200 Loss 3.3559 Accuracy 0.1055\n",
            "Epoch 4 Batch 250 Loss 3.3520 Accuracy 0.1055\n",
            "Epoch 4 Batch 300 Loss 3.3382 Accuracy 0.1060\n",
            "Epoch 4 Batch 350 Loss 3.3277 Accuracy 0.1062\n",
            "Epoch 4 Batch 400 Loss 3.3164 Accuracy 0.1066\n",
            "Epoch 4 Batch 450 Loss 3.3072 Accuracy 0.1067\n",
            "Epoch 4 Loss 3.2994 Accuracy 0.1069\n",
            "Time taken for 1 epoch: 68.96408462524414 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.2333 Accuracy 0.1128\n",
            "Epoch 5 Batch 50 Loss 3.1935 Accuracy 0.1109\n",
            "Epoch 5 Batch 100 Loss 3.1988 Accuracy 0.1114\n",
            "Epoch 5 Batch 150 Loss 3.1834 Accuracy 0.1116\n",
            "Epoch 5 Batch 200 Loss 3.1724 Accuracy 0.1114\n",
            "Epoch 5 Batch 250 Loss 3.1698 Accuracy 0.1112\n",
            "Epoch 5 Batch 300 Loss 3.1605 Accuracy 0.1115\n",
            "Epoch 5 Batch 350 Loss 3.1542 Accuracy 0.1116\n",
            "Epoch 5 Batch 400 Loss 3.1460 Accuracy 0.1117\n",
            "Epoch 5 Batch 450 Loss 3.1381 Accuracy 0.1119\n",
            "Epoch 5 Loss 3.1338 Accuracy 0.1120\n",
            "Time taken for 1 epoch: 68.93437957763672 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.9126 Accuracy 0.1152\n",
            "Epoch 6 Batch 50 Loss 3.0463 Accuracy 0.1146\n",
            "Epoch 6 Batch 100 Loss 3.0627 Accuracy 0.1152\n",
            "Epoch 6 Batch 150 Loss 3.0564 Accuracy 0.1144\n",
            "Epoch 6 Batch 200 Loss 3.0522 Accuracy 0.1143\n",
            "Epoch 6 Batch 250 Loss 3.0494 Accuracy 0.1144\n",
            "Epoch 6 Batch 300 Loss 3.0460 Accuracy 0.1145\n",
            "Epoch 6 Batch 350 Loss 3.0376 Accuracy 0.1147\n",
            "Epoch 6 Batch 400 Loss 3.0339 Accuracy 0.1148\n",
            "Epoch 6 Batch 450 Loss 3.0280 Accuracy 0.1149\n",
            "Epoch 6 Loss 3.0243 Accuracy 0.1150\n",
            "Time taken for 1 epoch: 68.92861413955688 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.9275 Accuracy 0.1138\n",
            "Epoch 7 Batch 50 Loss 2.9937 Accuracy 0.1154\n",
            "Epoch 7 Batch 100 Loss 2.9809 Accuracy 0.1162\n",
            "Epoch 7 Batch 150 Loss 2.9717 Accuracy 0.1159\n",
            "Epoch 7 Batch 200 Loss 2.9689 Accuracy 0.1158\n",
            "Epoch 7 Batch 250 Loss 2.9715 Accuracy 0.1159\n",
            "Epoch 7 Batch 300 Loss 2.9686 Accuracy 0.1162\n",
            "Epoch 7 Batch 350 Loss 2.9611 Accuracy 0.1164\n",
            "Epoch 7 Batch 400 Loss 2.9573 Accuracy 0.1164\n",
            "Epoch 7 Batch 450 Loss 2.9532 Accuracy 0.1165\n",
            "Epoch 7 Loss 2.9514 Accuracy 0.1164\n",
            "Time taken for 1 epoch: 68.92610812187195 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.9240 Accuracy 0.1152\n",
            "Epoch 8 Batch 50 Loss 2.9317 Accuracy 0.1168\n",
            "Epoch 8 Batch 100 Loss 2.9237 Accuracy 0.1181\n",
            "Epoch 8 Batch 150 Loss 2.9219 Accuracy 0.1174\n",
            "Epoch 8 Batch 200 Loss 2.9167 Accuracy 0.1170\n",
            "Epoch 8 Batch 250 Loss 2.9221 Accuracy 0.1169\n",
            "Epoch 8 Batch 300 Loss 2.9216 Accuracy 0.1171\n",
            "Epoch 8 Batch 350 Loss 2.9182 Accuracy 0.1171\n",
            "Epoch 8 Batch 400 Loss 2.9168 Accuracy 0.1170\n",
            "Epoch 8 Batch 450 Loss 2.9150 Accuracy 0.1169\n",
            "Epoch 8 Loss 2.9167 Accuracy 0.1169\n",
            "Time taken for 1 epoch: 68.93211054801941 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.8814 Accuracy 0.1201\n",
            "Epoch 9 Batch 50 Loss 2.9156 Accuracy 0.1165\n",
            "Epoch 9 Batch 100 Loss 2.8941 Accuracy 0.1182\n",
            "Epoch 9 Batch 150 Loss 2.8933 Accuracy 0.1177\n",
            "Epoch 9 Batch 200 Loss 2.8871 Accuracy 0.1176\n",
            "Epoch 9 Batch 250 Loss 2.8880 Accuracy 0.1176\n",
            "Epoch 9 Batch 300 Loss 2.8835 Accuracy 0.1178\n",
            "Epoch 9 Batch 350 Loss 2.8756 Accuracy 0.1180\n",
            "Epoch 9 Batch 400 Loss 2.8666 Accuracy 0.1181\n",
            "Epoch 9 Batch 450 Loss 2.8608 Accuracy 0.1184\n",
            "Epoch 9 Loss 2.8573 Accuracy 0.1184\n",
            "Time taken for 1 epoch: 69.03766012191772 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.7713 Accuracy 0.1250\n",
            "Epoch 10 Batch 50 Loss 2.8128 Accuracy 0.1195\n",
            "Epoch 10 Batch 100 Loss 2.7963 Accuracy 0.1213\n",
            "Epoch 10 Batch 150 Loss 2.7821 Accuracy 0.1214\n",
            "Epoch 10 Batch 200 Loss 2.7782 Accuracy 0.1209\n",
            "Epoch 10 Batch 250 Loss 2.7772 Accuracy 0.1208\n",
            "Epoch 10 Batch 300 Loss 2.7698 Accuracy 0.1213\n",
            "Epoch 10 Batch 350 Loss 2.7645 Accuracy 0.1215\n",
            "Epoch 10 Batch 400 Loss 2.7576 Accuracy 0.1216\n",
            "Epoch 10 Batch 450 Loss 2.7514 Accuracy 0.1218\n",
            "Epoch 10 Loss 2.7486 Accuracy 0.1219\n",
            "Time taken for 1 epoch: 68.9593415260315 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.7002 Accuracy 0.1221\n",
            "Epoch 11 Batch 50 Loss 2.6993 Accuracy 0.1229\n",
            "Epoch 11 Batch 100 Loss 2.6924 Accuracy 0.1245\n",
            "Epoch 11 Batch 150 Loss 2.6854 Accuracy 0.1242\n",
            "Epoch 11 Batch 200 Loss 2.6790 Accuracy 0.1239\n",
            "Epoch 11 Batch 250 Loss 2.6770 Accuracy 0.1241\n",
            "Epoch 11 Batch 300 Loss 2.6707 Accuracy 0.1246\n",
            "Epoch 11 Batch 350 Loss 2.6643 Accuracy 0.1248\n",
            "Epoch 11 Batch 400 Loss 2.6557 Accuracy 0.1249\n",
            "Epoch 11 Batch 450 Loss 2.6497 Accuracy 0.1252\n",
            "Epoch 11 Loss 2.6472 Accuracy 0.1253\n",
            "Time taken for 1 epoch: 68.97745752334595 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.6352 Accuracy 0.1182\n",
            "Epoch 12 Batch 50 Loss 2.5913 Accuracy 0.1271\n",
            "Epoch 12 Batch 100 Loss 2.5767 Accuracy 0.1292\n",
            "Epoch 12 Batch 150 Loss 2.5728 Accuracy 0.1285\n",
            "Epoch 12 Batch 200 Loss 2.5693 Accuracy 0.1282\n",
            "Epoch 12 Batch 250 Loss 2.5682 Accuracy 0.1283\n",
            "Epoch 12 Batch 300 Loss 2.5658 Accuracy 0.1285\n",
            "Epoch 12 Batch 350 Loss 2.5574 Accuracy 0.1288\n",
            "Epoch 12 Batch 400 Loss 2.5513 Accuracy 0.1289\n",
            "Epoch 12 Batch 450 Loss 2.5456 Accuracy 0.1292\n",
            "Epoch 12 Loss 2.5414 Accuracy 0.1293\n",
            "Time taken for 1 epoch: 68.95891952514648 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.4665 Accuracy 0.1338\n",
            "Epoch 13 Batch 50 Loss 2.4864 Accuracy 0.1318\n",
            "Epoch 13 Batch 100 Loss 2.4864 Accuracy 0.1336\n",
            "Epoch 13 Batch 150 Loss 2.4796 Accuracy 0.1325\n",
            "Epoch 13 Batch 200 Loss 2.4747 Accuracy 0.1323\n",
            "Epoch 13 Batch 250 Loss 2.4708 Accuracy 0.1325\n",
            "Epoch 13 Batch 300 Loss 2.4674 Accuracy 0.1326\n",
            "Epoch 13 Batch 350 Loss 2.4638 Accuracy 0.1326\n",
            "Epoch 13 Batch 400 Loss 2.4586 Accuracy 0.1326\n",
            "Epoch 13 Batch 450 Loss 2.4535 Accuracy 0.1328\n",
            "Epoch 13 Loss 2.4502 Accuracy 0.1329\n",
            "Time taken for 1 epoch: 69.07995510101318 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.3156 Accuracy 0.1416\n",
            "Epoch 14 Batch 50 Loss 2.3979 Accuracy 0.1358\n",
            "Epoch 14 Batch 100 Loss 2.3887 Accuracy 0.1371\n",
            "Epoch 14 Batch 150 Loss 2.3841 Accuracy 0.1363\n",
            "Epoch 14 Batch 200 Loss 2.3762 Accuracy 0.1361\n",
            "Epoch 14 Batch 250 Loss 2.3739 Accuracy 0.1360\n",
            "Epoch 14 Batch 300 Loss 2.3698 Accuracy 0.1364\n",
            "Epoch 14 Batch 350 Loss 2.3615 Accuracy 0.1367\n",
            "Epoch 14 Batch 400 Loss 2.3574 Accuracy 0.1367\n",
            "Epoch 14 Batch 450 Loss 2.3532 Accuracy 0.1370\n",
            "Epoch 14 Loss 2.3497 Accuracy 0.1371\n",
            "Time taken for 1 epoch: 69.03174066543579 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.2430 Accuracy 0.1523\n",
            "Epoch 15 Batch 50 Loss 2.3108 Accuracy 0.1374\n",
            "Epoch 15 Batch 100 Loss 2.3005 Accuracy 0.1396\n",
            "Epoch 15 Batch 150 Loss 2.2933 Accuracy 0.1395\n",
            "Epoch 15 Batch 200 Loss 2.2922 Accuracy 0.1393\n",
            "Epoch 15 Batch 250 Loss 2.2898 Accuracy 0.1395\n",
            "Epoch 15 Batch 300 Loss 2.2836 Accuracy 0.1400\n",
            "Epoch 15 Batch 350 Loss 2.2776 Accuracy 0.1403\n",
            "Epoch 15 Batch 400 Loss 2.2719 Accuracy 0.1405\n",
            "Epoch 15 Batch 450 Loss 2.2668 Accuracy 0.1406\n",
            "Epoch 15 Loss 2.2633 Accuracy 0.1408\n",
            "Time taken for 1 epoch: 69.05410599708557 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.1969 Accuracy 0.1362\n",
            "Epoch 16 Batch 50 Loss 2.2114 Accuracy 0.1422\n",
            "Epoch 16 Batch 100 Loss 2.2130 Accuracy 0.1430\n",
            "Epoch 16 Batch 150 Loss 2.2039 Accuracy 0.1433\n",
            "Epoch 16 Batch 200 Loss 2.2028 Accuracy 0.1431\n",
            "Epoch 16 Batch 250 Loss 2.2018 Accuracy 0.1434\n",
            "Epoch 16 Batch 300 Loss 2.1997 Accuracy 0.1438\n",
            "Epoch 16 Batch 350 Loss 2.1949 Accuracy 0.1438\n",
            "Epoch 16 Batch 400 Loss 2.1881 Accuracy 0.1441\n",
            "Epoch 16 Batch 450 Loss 2.1838 Accuracy 0.1443\n",
            "Epoch 16 Loss 2.1797 Accuracy 0.1446\n",
            "Time taken for 1 epoch: 69.04119634628296 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.1313 Accuracy 0.1523\n",
            "Epoch 17 Batch 50 Loss 2.1351 Accuracy 0.1470\n",
            "Epoch 17 Batch 100 Loss 2.1349 Accuracy 0.1470\n",
            "Epoch 17 Batch 150 Loss 2.1282 Accuracy 0.1473\n",
            "Epoch 17 Batch 200 Loss 2.1225 Accuracy 0.1470\n",
            "Epoch 17 Batch 250 Loss 2.1188 Accuracy 0.1472\n",
            "Epoch 17 Batch 300 Loss 2.1138 Accuracy 0.1477\n",
            "Epoch 17 Batch 350 Loss 2.1088 Accuracy 0.1481\n",
            "Epoch 17 Batch 400 Loss 2.1032 Accuracy 0.1484\n",
            "Epoch 17 Batch 450 Loss 2.0970 Accuracy 0.1486\n",
            "Epoch 17 Loss 2.0933 Accuracy 0.1488\n",
            "Time taken for 1 epoch: 68.96659779548645 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.0324 Accuracy 0.1558\n",
            "Epoch 18 Batch 50 Loss 2.0341 Accuracy 0.1512\n",
            "Epoch 18 Batch 100 Loss 2.0327 Accuracy 0.1522\n",
            "Epoch 18 Batch 150 Loss 2.0336 Accuracy 0.1519\n",
            "Epoch 18 Batch 200 Loss 2.0296 Accuracy 0.1517\n",
            "Epoch 18 Batch 250 Loss 2.0293 Accuracy 0.1519\n",
            "Epoch 18 Batch 300 Loss 2.0246 Accuracy 0.1523\n",
            "Epoch 18 Batch 350 Loss 2.0207 Accuracy 0.1524\n",
            "Epoch 18 Batch 400 Loss 2.0166 Accuracy 0.1526\n",
            "Epoch 18 Batch 450 Loss 2.0128 Accuracy 0.1528\n",
            "Epoch 18 Loss 2.0082 Accuracy 0.1531\n",
            "Time taken for 1 epoch: 69.02350497245789 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.9003 Accuracy 0.1606\n",
            "Epoch 19 Batch 50 Loss 1.9620 Accuracy 0.1566\n",
            "Epoch 19 Batch 100 Loss 1.9556 Accuracy 0.1568\n",
            "Epoch 19 Batch 150 Loss 1.9496 Accuracy 0.1568\n",
            "Epoch 19 Batch 200 Loss 1.9464 Accuracy 0.1565\n",
            "Epoch 19 Batch 250 Loss 1.9452 Accuracy 0.1565\n",
            "Epoch 19 Batch 300 Loss 1.9392 Accuracy 0.1569\n",
            "Epoch 19 Batch 350 Loss 1.9352 Accuracy 0.1571\n",
            "Epoch 19 Batch 400 Loss 1.9313 Accuracy 0.1571\n",
            "Epoch 19 Batch 450 Loss 1.9282 Accuracy 0.1573\n",
            "Epoch 19 Loss 1.9240 Accuracy 0.1574\n",
            "Time taken for 1 epoch: 68.9755368232727 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.8485 Accuracy 0.1528\n",
            "Epoch 20 Batch 50 Loss 1.8750 Accuracy 0.1601\n",
            "Epoch 20 Batch 100 Loss 1.8736 Accuracy 0.1612\n",
            "Epoch 20 Batch 150 Loss 1.8740 Accuracy 0.1611\n",
            "Epoch 20 Batch 200 Loss 1.8756 Accuracy 0.1605\n",
            "Epoch 20 Batch 250 Loss 1.8723 Accuracy 0.1607\n",
            "Epoch 20 Batch 300 Loss 1.8670 Accuracy 0.1611\n",
            "Epoch 20 Batch 350 Loss 1.8630 Accuracy 0.1612\n",
            "Epoch 20 Batch 400 Loss 1.8584 Accuracy 0.1614\n",
            "Epoch 20 Batch 450 Loss 1.8534 Accuracy 0.1617\n",
            "Epoch 20 Loss 1.8500 Accuracy 0.1619\n",
            "Time taken for 1 epoch: 69.0095067024231 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.8224 Accuracy 0.1597\n",
            "Epoch 21 Batch 50 Loss 1.8023 Accuracy 0.1651\n",
            "Epoch 21 Batch 100 Loss 1.7982 Accuracy 0.1656\n",
            "Epoch 21 Batch 150 Loss 1.7954 Accuracy 0.1653\n",
            "Epoch 21 Batch 200 Loss 1.7915 Accuracy 0.1647\n",
            "Epoch 21 Batch 250 Loss 1.7896 Accuracy 0.1651\n",
            "Epoch 21 Batch 300 Loss 1.7879 Accuracy 0.1654\n",
            "Epoch 21 Batch 350 Loss 1.7831 Accuracy 0.1656\n",
            "Epoch 21 Batch 400 Loss 1.7804 Accuracy 0.1659\n",
            "Epoch 21 Batch 450 Loss 1.7772 Accuracy 0.1659\n",
            "Epoch 21 Loss 1.7736 Accuracy 0.1662\n",
            "Time taken for 1 epoch: 69.01730442047119 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.8638 Accuracy 0.1606\n",
            "Epoch 22 Batch 50 Loss 1.7355 Accuracy 0.1691\n",
            "Epoch 22 Batch 100 Loss 1.7395 Accuracy 0.1695\n",
            "Epoch 22 Batch 150 Loss 1.7328 Accuracy 0.1695\n",
            "Epoch 22 Batch 200 Loss 1.7277 Accuracy 0.1692\n",
            "Epoch 22 Batch 250 Loss 1.7268 Accuracy 0.1692\n",
            "Epoch 22 Batch 300 Loss 1.7238 Accuracy 0.1697\n",
            "Epoch 22 Batch 350 Loss 1.7196 Accuracy 0.1698\n",
            "Epoch 22 Batch 400 Loss 1.7140 Accuracy 0.1701\n",
            "Epoch 22 Batch 450 Loss 1.7125 Accuracy 0.1702\n",
            "Epoch 22 Loss 1.7091 Accuracy 0.1704\n",
            "Time taken for 1 epoch: 68.96925234794617 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.7632 Accuracy 0.1636\n",
            "Epoch 23 Batch 50 Loss 1.6622 Accuracy 0.1742\n",
            "Epoch 23 Batch 100 Loss 1.6614 Accuracy 0.1740\n",
            "Epoch 23 Batch 150 Loss 1.6606 Accuracy 0.1740\n",
            "Epoch 23 Batch 200 Loss 1.6587 Accuracy 0.1734\n",
            "Epoch 23 Batch 250 Loss 1.6590 Accuracy 0.1736\n",
            "Epoch 23 Batch 300 Loss 1.6556 Accuracy 0.1739\n",
            "Epoch 23 Batch 350 Loss 1.6527 Accuracy 0.1742\n",
            "Epoch 23 Batch 400 Loss 1.6489 Accuracy 0.1742\n",
            "Epoch 23 Batch 450 Loss 1.6454 Accuracy 0.1745\n",
            "Epoch 23 Loss 1.6419 Accuracy 0.1746\n",
            "Time taken for 1 epoch: 68.95828437805176 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.6336 Accuracy 0.1694\n",
            "Epoch 24 Batch 50 Loss 1.5976 Accuracy 0.1774\n",
            "Epoch 24 Batch 100 Loss 1.5968 Accuracy 0.1787\n",
            "Epoch 24 Batch 150 Loss 1.5939 Accuracy 0.1782\n",
            "Epoch 24 Batch 200 Loss 1.5880 Accuracy 0.1779\n",
            "Epoch 24 Batch 250 Loss 1.5899 Accuracy 0.1779\n",
            "Epoch 24 Batch 300 Loss 1.5876 Accuracy 0.1785\n",
            "Epoch 24 Batch 350 Loss 1.5856 Accuracy 0.1786\n",
            "Epoch 24 Batch 400 Loss 1.5807 Accuracy 0.1787\n",
            "Epoch 24 Batch 450 Loss 1.5797 Accuracy 0.1788\n",
            "Epoch 24 Loss 1.5769 Accuracy 0.1789\n",
            "Time taken for 1 epoch: 68.89885020256042 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.5674 Accuracy 0.1816\n",
            "Epoch 25 Batch 50 Loss 1.5370 Accuracy 0.1814\n",
            "Epoch 25 Batch 100 Loss 1.5320 Accuracy 0.1827\n",
            "Epoch 25 Batch 150 Loss 1.5309 Accuracy 0.1824\n",
            "Epoch 25 Batch 200 Loss 1.5308 Accuracy 0.1817\n",
            "Epoch 25 Batch 250 Loss 1.5328 Accuracy 0.1818\n",
            "Epoch 25 Batch 300 Loss 1.5304 Accuracy 0.1820\n",
            "Epoch 25 Batch 350 Loss 1.5271 Accuracy 0.1822\n",
            "Epoch 25 Batch 400 Loss 1.5229 Accuracy 0.1823\n",
            "Epoch 25 Batch 450 Loss 1.5195 Accuracy 0.1826\n",
            "Epoch 25 Loss 1.5162 Accuracy 0.1828\n",
            "Time taken for 1 epoch: 69.11350870132446 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.5595 Accuracy 0.1880\n",
            "Epoch 26 Batch 50 Loss 1.4836 Accuracy 0.1854\n",
            "Epoch 26 Batch 100 Loss 1.4736 Accuracy 0.1866\n",
            "Epoch 26 Batch 150 Loss 1.4723 Accuracy 0.1864\n",
            "Epoch 26 Batch 200 Loss 1.4709 Accuracy 0.1859\n",
            "Epoch 26 Batch 250 Loss 1.4712 Accuracy 0.1856\n",
            "Epoch 26 Batch 300 Loss 1.4676 Accuracy 0.1862\n",
            "Epoch 26 Batch 350 Loss 1.4653 Accuracy 0.1864\n",
            "Epoch 26 Batch 400 Loss 1.4623 Accuracy 0.1864\n",
            "Epoch 26 Batch 450 Loss 1.4580 Accuracy 0.1868\n",
            "Epoch 26 Loss 1.4552 Accuracy 0.1869\n",
            "Time taken for 1 epoch: 68.9782292842865 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.3807 Accuracy 0.1826\n",
            "Epoch 27 Batch 50 Loss 1.4295 Accuracy 0.1888\n",
            "Epoch 27 Batch 100 Loss 1.4229 Accuracy 0.1910\n",
            "Epoch 27 Batch 150 Loss 1.4222 Accuracy 0.1898\n",
            "Epoch 27 Batch 200 Loss 1.4209 Accuracy 0.1897\n",
            "Epoch 27 Batch 250 Loss 1.4210 Accuracy 0.1894\n",
            "Epoch 27 Batch 300 Loss 1.4176 Accuracy 0.1898\n",
            "Epoch 27 Batch 350 Loss 1.4150 Accuracy 0.1899\n",
            "Epoch 27 Batch 400 Loss 1.4118 Accuracy 0.1899\n",
            "Epoch 27 Batch 450 Loss 1.4074 Accuracy 0.1903\n",
            "Epoch 27 Loss 1.4046 Accuracy 0.1905\n",
            "Time taken for 1 epoch: 68.93923544883728 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.4368 Accuracy 0.1909\n",
            "Epoch 28 Batch 50 Loss 1.3768 Accuracy 0.1928\n",
            "Epoch 28 Batch 100 Loss 1.3756 Accuracy 0.1934\n",
            "Epoch 28 Batch 150 Loss 1.3706 Accuracy 0.1931\n",
            "Epoch 28 Batch 200 Loss 1.3714 Accuracy 0.1926\n",
            "Epoch 28 Batch 250 Loss 1.3699 Accuracy 0.1926\n",
            "Epoch 28 Batch 300 Loss 1.3657 Accuracy 0.1932\n",
            "Epoch 28 Batch 350 Loss 1.3627 Accuracy 0.1934\n",
            "Epoch 28 Batch 400 Loss 1.3587 Accuracy 0.1937\n",
            "Epoch 28 Batch 450 Loss 1.3559 Accuracy 0.1940\n",
            "Epoch 28 Loss 1.3523 Accuracy 0.1942\n",
            "Time taken for 1 epoch: 68.89749956130981 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.3666 Accuracy 0.1836\n",
            "Epoch 29 Batch 50 Loss 1.3236 Accuracy 0.1970\n",
            "Epoch 29 Batch 100 Loss 1.3264 Accuracy 0.1973\n",
            "Epoch 29 Batch 150 Loss 1.3213 Accuracy 0.1973\n",
            "Epoch 29 Batch 200 Loss 1.3199 Accuracy 0.1966\n",
            "Epoch 29 Batch 250 Loss 1.3198 Accuracy 0.1967\n",
            "Epoch 29 Batch 300 Loss 1.3172 Accuracy 0.1971\n",
            "Epoch 29 Batch 350 Loss 1.3157 Accuracy 0.1969\n",
            "Epoch 29 Batch 400 Loss 1.3123 Accuracy 0.1972\n",
            "Epoch 29 Batch 450 Loss 1.3094 Accuracy 0.1974\n",
            "Epoch 29 Loss 1.3063 Accuracy 0.1976\n",
            "Time taken for 1 epoch: 68.91223120689392 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.3134 Accuracy 0.1865\n",
            "Epoch 30 Batch 50 Loss 1.2775 Accuracy 0.2001\n",
            "Epoch 30 Batch 100 Loss 1.2753 Accuracy 0.2008\n",
            "Epoch 30 Batch 150 Loss 1.2743 Accuracy 0.2005\n",
            "Epoch 30 Batch 200 Loss 1.2723 Accuracy 0.2000\n",
            "Epoch 30 Batch 250 Loss 1.2717 Accuracy 0.2003\n",
            "Epoch 30 Batch 300 Loss 1.2721 Accuracy 0.2005\n",
            "Epoch 30 Batch 350 Loss 1.2702 Accuracy 0.2006\n",
            "Epoch 30 Batch 400 Loss 1.2677 Accuracy 0.2008\n",
            "Epoch 30 Batch 450 Loss 1.2650 Accuracy 0.2010\n",
            "Epoch 30 Loss 1.2624 Accuracy 0.2012\n",
            "Time taken for 1 epoch: 68.87959122657776 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.3328 Accuracy 0.2046\n",
            "Epoch 31 Batch 50 Loss 1.2264 Accuracy 0.2042\n",
            "Epoch 31 Batch 100 Loss 1.2274 Accuracy 0.2045\n",
            "Epoch 31 Batch 150 Loss 1.2292 Accuracy 0.2042\n",
            "Epoch 31 Batch 200 Loss 1.2252 Accuracy 0.2038\n",
            "Epoch 31 Batch 250 Loss 1.2249 Accuracy 0.2038\n",
            "Epoch 31 Batch 300 Loss 1.2233 Accuracy 0.2041\n",
            "Epoch 31 Batch 350 Loss 1.2213 Accuracy 0.2044\n",
            "Epoch 31 Batch 400 Loss 1.2182 Accuracy 0.2044\n",
            "Epoch 31 Batch 450 Loss 1.2171 Accuracy 0.2044\n",
            "Epoch 31 Loss 1.2147 Accuracy 0.2045\n",
            "Time taken for 1 epoch: 68.91781854629517 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.2487 Accuracy 0.1812\n",
            "Epoch 32 Batch 50 Loss 1.1924 Accuracy 0.2072\n",
            "Epoch 32 Batch 100 Loss 1.1878 Accuracy 0.2081\n",
            "Epoch 32 Batch 150 Loss 1.1849 Accuracy 0.2077\n",
            "Epoch 32 Batch 200 Loss 1.1833 Accuracy 0.2075\n",
            "Epoch 32 Batch 250 Loss 1.1837 Accuracy 0.2073\n",
            "Epoch 32 Batch 300 Loss 1.1844 Accuracy 0.2074\n",
            "Epoch 32 Batch 350 Loss 1.1839 Accuracy 0.2075\n",
            "Epoch 32 Batch 400 Loss 1.1812 Accuracy 0.2074\n",
            "Epoch 32 Batch 450 Loss 1.1793 Accuracy 0.2075\n",
            "Epoch 32 Loss 1.1773 Accuracy 0.2077\n",
            "Time taken for 1 epoch: 68.8500144481659 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.3105 Accuracy 0.1899\n",
            "Epoch 33 Batch 50 Loss 1.1583 Accuracy 0.2095\n",
            "Epoch 33 Batch 100 Loss 1.1543 Accuracy 0.2102\n",
            "Epoch 33 Batch 150 Loss 1.1543 Accuracy 0.2098\n",
            "Epoch 33 Batch 200 Loss 1.1526 Accuracy 0.2093\n",
            "Epoch 33 Batch 250 Loss 1.1521 Accuracy 0.2095\n",
            "Epoch 33 Batch 300 Loss 1.1508 Accuracy 0.2098\n",
            "Epoch 33 Batch 350 Loss 1.1485 Accuracy 0.2098\n",
            "Epoch 33 Batch 400 Loss 1.1465 Accuracy 0.2100\n",
            "Epoch 33 Batch 450 Loss 1.1428 Accuracy 0.2102\n",
            "Epoch 33 Loss 1.1403 Accuracy 0.2104\n",
            "Time taken for 1 epoch: 68.84844589233398 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.2286 Accuracy 0.2207\n",
            "Epoch 34 Batch 50 Loss 1.1136 Accuracy 0.2130\n",
            "Epoch 34 Batch 100 Loss 1.1088 Accuracy 0.2134\n",
            "Epoch 34 Batch 150 Loss 1.1071 Accuracy 0.2135\n",
            "Epoch 34 Batch 200 Loss 1.1055 Accuracy 0.2131\n",
            "Epoch 34 Batch 250 Loss 1.1050 Accuracy 0.2132\n",
            "Epoch 34 Batch 300 Loss 1.1049 Accuracy 0.2134\n",
            "Epoch 34 Batch 350 Loss 1.1037 Accuracy 0.2134\n",
            "Epoch 34 Batch 400 Loss 1.1017 Accuracy 0.2135\n",
            "Epoch 34 Batch 450 Loss 1.0984 Accuracy 0.2138\n",
            "Epoch 34 Loss 1.0957 Accuracy 0.2140\n",
            "Time taken for 1 epoch: 68.90997886657715 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.1222 Accuracy 0.1895\n",
            "Epoch 35 Batch 50 Loss 1.0767 Accuracy 0.2161\n",
            "Epoch 35 Batch 100 Loss 1.0768 Accuracy 0.2163\n",
            "Epoch 35 Batch 150 Loss 1.0750 Accuracy 0.2163\n",
            "Epoch 35 Batch 200 Loss 1.0739 Accuracy 0.2159\n",
            "Epoch 35 Batch 250 Loss 1.0731 Accuracy 0.2158\n",
            "Epoch 35 Batch 300 Loss 1.0715 Accuracy 0.2162\n",
            "Epoch 35 Batch 350 Loss 1.0699 Accuracy 0.2160\n",
            "Epoch 35 Batch 400 Loss 1.0685 Accuracy 0.2161\n",
            "Epoch 35 Batch 450 Loss 1.0669 Accuracy 0.2163\n",
            "Epoch 35 Loss 1.0646 Accuracy 0.2164\n",
            "Time taken for 1 epoch: 68.93231177330017 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.1309 Accuracy 0.1919\n",
            "Epoch 36 Batch 50 Loss 1.0369 Accuracy 0.2197\n",
            "Epoch 36 Batch 100 Loss 1.0366 Accuracy 0.2203\n",
            "Epoch 36 Batch 150 Loss 1.0402 Accuracy 0.2197\n",
            "Epoch 36 Batch 200 Loss 1.0372 Accuracy 0.2189\n",
            "Epoch 36 Batch 250 Loss 1.0355 Accuracy 0.2189\n",
            "Epoch 36 Batch 300 Loss 1.0335 Accuracy 0.2192\n",
            "Epoch 36 Batch 350 Loss 1.0342 Accuracy 0.2191\n",
            "Epoch 36 Batch 400 Loss 1.0320 Accuracy 0.2192\n",
            "Epoch 36 Batch 450 Loss 1.0296 Accuracy 0.2194\n",
            "Epoch 36 Loss 1.0283 Accuracy 0.2194\n",
            "Time taken for 1 epoch: 68.87331199645996 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.9714 Accuracy 0.2339\n",
            "Epoch 37 Batch 50 Loss 1.0080 Accuracy 0.2221\n",
            "Epoch 37 Batch 100 Loss 1.0065 Accuracy 0.2228\n",
            "Epoch 37 Batch 150 Loss 1.0048 Accuracy 0.2223\n",
            "Epoch 37 Batch 200 Loss 1.0044 Accuracy 0.2211\n",
            "Epoch 37 Batch 250 Loss 1.0038 Accuracy 0.2214\n",
            "Epoch 37 Batch 300 Loss 1.0022 Accuracy 0.2217\n",
            "Epoch 37 Batch 350 Loss 1.0002 Accuracy 0.2219\n",
            "Epoch 37 Batch 400 Loss 0.9988 Accuracy 0.2220\n",
            "Epoch 37 Batch 450 Loss 0.9988 Accuracy 0.2221\n",
            "Epoch 37 Loss 0.9970 Accuracy 0.2220\n",
            "Time taken for 1 epoch: 68.88305759429932 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.9857 Accuracy 0.2178\n",
            "Epoch 38 Batch 50 Loss 0.9747 Accuracy 0.2245\n",
            "Epoch 38 Batch 100 Loss 0.9703 Accuracy 0.2257\n",
            "Epoch 38 Batch 150 Loss 0.9722 Accuracy 0.2255\n",
            "Epoch 38 Batch 200 Loss 0.9709 Accuracy 0.2245\n",
            "Epoch 38 Batch 250 Loss 0.9699 Accuracy 0.2248\n",
            "Epoch 38 Batch 300 Loss 0.9705 Accuracy 0.2248\n",
            "Epoch 38 Batch 350 Loss 0.9700 Accuracy 0.2249\n",
            "Epoch 38 Batch 400 Loss 0.9677 Accuracy 0.2248\n",
            "Epoch 38 Batch 450 Loss 0.9670 Accuracy 0.2251\n",
            "Epoch 38 Loss 0.9655 Accuracy 0.2250\n",
            "Time taken for 1 epoch: 68.92615675926208 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 1.0138 Accuracy 0.2207\n",
            "Epoch 39 Batch 50 Loss 0.9555 Accuracy 0.2266\n",
            "Epoch 39 Batch 100 Loss 0.9486 Accuracy 0.2276\n",
            "Epoch 39 Batch 150 Loss 0.9514 Accuracy 0.2265\n",
            "Epoch 39 Batch 200 Loss 0.9498 Accuracy 0.2262\n",
            "Epoch 39 Batch 250 Loss 0.9499 Accuracy 0.2263\n",
            "Epoch 39 Batch 300 Loss 0.9473 Accuracy 0.2263\n",
            "Epoch 39 Batch 350 Loss 0.9484 Accuracy 0.2264\n",
            "Epoch 39 Batch 400 Loss 0.9465 Accuracy 0.2263\n",
            "Epoch 39 Batch 450 Loss 0.9454 Accuracy 0.2265\n",
            "Epoch 39 Loss 0.9429 Accuracy 0.2266\n",
            "Time taken for 1 epoch: 68.94041466712952 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 1.0712 Accuracy 0.1987\n",
            "Epoch 40 Batch 50 Loss 0.9138 Accuracy 0.2300\n",
            "Epoch 40 Batch 100 Loss 0.9124 Accuracy 0.2306\n",
            "Epoch 40 Batch 150 Loss 0.9129 Accuracy 0.2299\n",
            "Epoch 40 Batch 200 Loss 0.9154 Accuracy 0.2289\n",
            "Epoch 40 Batch 250 Loss 0.9158 Accuracy 0.2292\n",
            "Epoch 40 Batch 300 Loss 0.9146 Accuracy 0.2294\n",
            "Epoch 40 Batch 350 Loss 0.9140 Accuracy 0.2293\n",
            "Epoch 40 Batch 400 Loss 0.9125 Accuracy 0.2293\n",
            "Epoch 40 Batch 450 Loss 0.9110 Accuracy 0.2294\n",
            "Epoch 40 Loss 0.9098 Accuracy 0.2294\n",
            "Time taken for 1 epoch: 68.94493460655212 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.9498 Accuracy 0.2305\n",
            "Epoch 41 Batch 50 Loss 0.8878 Accuracy 0.2314\n",
            "Epoch 41 Batch 100 Loss 0.8903 Accuracy 0.2319\n",
            "Epoch 41 Batch 150 Loss 0.8912 Accuracy 0.2320\n",
            "Epoch 41 Batch 200 Loss 0.8904 Accuracy 0.2314\n",
            "Epoch 41 Batch 250 Loss 0.8891 Accuracy 0.2312\n",
            "Epoch 41 Batch 300 Loss 0.8900 Accuracy 0.2314\n",
            "Epoch 41 Batch 350 Loss 0.8900 Accuracy 0.2314\n",
            "Epoch 41 Batch 400 Loss 0.8893 Accuracy 0.2313\n",
            "Epoch 41 Batch 450 Loss 0.8891 Accuracy 0.2313\n",
            "Epoch 41 Loss 0.8870 Accuracy 0.2314\n",
            "Time taken for 1 epoch: 68.9231288433075 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.9683 Accuracy 0.2183\n",
            "Epoch 42 Batch 50 Loss 0.8717 Accuracy 0.2339\n",
            "Epoch 42 Batch 100 Loss 0.8724 Accuracy 0.2339\n",
            "Epoch 42 Batch 150 Loss 0.8718 Accuracy 0.2334\n",
            "Epoch 42 Batch 200 Loss 0.8724 Accuracy 0.2325\n",
            "Epoch 42 Batch 250 Loss 0.8718 Accuracy 0.2331\n",
            "Epoch 42 Batch 300 Loss 0.8721 Accuracy 0.2330\n",
            "Epoch 42 Batch 350 Loss 0.8728 Accuracy 0.2330\n",
            "Epoch 42 Batch 400 Loss 0.8706 Accuracy 0.2329\n",
            "Epoch 42 Batch 450 Loss 0.8692 Accuracy 0.2331\n",
            "Epoch 42 Loss 0.8667 Accuracy 0.2333\n",
            "Time taken for 1 epoch: 68.92948842048645 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.8709 Accuracy 0.2397\n",
            "Epoch 43 Batch 50 Loss 0.8584 Accuracy 0.2347\n",
            "Epoch 43 Batch 100 Loss 0.8491 Accuracy 0.2362\n",
            "Epoch 43 Batch 150 Loss 0.8481 Accuracy 0.2361\n",
            "Epoch 43 Batch 200 Loss 0.8448 Accuracy 0.2353\n",
            "Epoch 43 Batch 250 Loss 0.8450 Accuracy 0.2353\n",
            "Epoch 43 Batch 300 Loss 0.8458 Accuracy 0.2355\n",
            "Epoch 43 Batch 350 Loss 0.8445 Accuracy 0.2357\n",
            "Epoch 43 Batch 400 Loss 0.8435 Accuracy 0.2355\n",
            "Epoch 43 Batch 450 Loss 0.8429 Accuracy 0.2356\n",
            "Epoch 43 Loss 0.8412 Accuracy 0.2357\n",
            "Time taken for 1 epoch: 68.9270007610321 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.8614 Accuracy 0.2383\n",
            "Epoch 44 Batch 50 Loss 0.8241 Accuracy 0.2383\n",
            "Epoch 44 Batch 100 Loss 0.8240 Accuracy 0.2386\n",
            "Epoch 44 Batch 150 Loss 0.8226 Accuracy 0.2383\n",
            "Epoch 44 Batch 200 Loss 0.8210 Accuracy 0.2373\n",
            "Epoch 44 Batch 250 Loss 0.8201 Accuracy 0.2378\n",
            "Epoch 44 Batch 300 Loss 0.8216 Accuracy 0.2377\n",
            "Epoch 44 Batch 350 Loss 0.8216 Accuracy 0.2375\n",
            "Epoch 44 Batch 400 Loss 0.8212 Accuracy 0.2373\n",
            "Epoch 44 Batch 450 Loss 0.8206 Accuracy 0.2373\n",
            "Epoch 44 Loss 0.8187 Accuracy 0.2374\n",
            "Time taken for 1 epoch: 81.9187388420105 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.8433 Accuracy 0.2407\n",
            "Epoch 45 Batch 50 Loss 0.8061 Accuracy 0.2391\n",
            "Epoch 45 Batch 100 Loss 0.8021 Accuracy 0.2407\n",
            "Epoch 45 Batch 150 Loss 0.8053 Accuracy 0.2397\n",
            "Epoch 45 Batch 200 Loss 0.8056 Accuracy 0.2391\n",
            "Epoch 45 Batch 250 Loss 0.8045 Accuracy 0.2393\n",
            "Epoch 45 Batch 300 Loss 0.8024 Accuracy 0.2395\n",
            "Epoch 45 Batch 350 Loss 0.8016 Accuracy 0.2395\n",
            "Epoch 45 Batch 400 Loss 0.8001 Accuracy 0.2393\n",
            "Epoch 45 Batch 450 Loss 0.7990 Accuracy 0.2395\n",
            "Epoch 45 Loss 0.7977 Accuracy 0.2396\n",
            "Time taken for 1 epoch: 68.90583515167236 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.8628 Accuracy 0.2285\n",
            "Epoch 46 Batch 50 Loss 0.7774 Accuracy 0.2434\n",
            "Epoch 46 Batch 100 Loss 0.7840 Accuracy 0.2418\n",
            "Epoch 46 Batch 150 Loss 0.7824 Accuracy 0.2421\n",
            "Epoch 46 Batch 200 Loss 0.7831 Accuracy 0.2407\n",
            "Epoch 46 Batch 250 Loss 0.7827 Accuracy 0.2409\n",
            "Epoch 46 Batch 300 Loss 0.7832 Accuracy 0.2409\n",
            "Epoch 46 Batch 350 Loss 0.7821 Accuracy 0.2412\n",
            "Epoch 46 Batch 400 Loss 0.7812 Accuracy 0.2411\n",
            "Epoch 46 Batch 450 Loss 0.7808 Accuracy 0.2411\n",
            "Epoch 46 Loss 0.7783 Accuracy 0.2412\n",
            "Time taken for 1 epoch: 68.92749261856079 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.8400 Accuracy 0.2266\n",
            "Epoch 47 Batch 50 Loss 0.7718 Accuracy 0.2421\n",
            "Epoch 47 Batch 100 Loss 0.7646 Accuracy 0.2436\n",
            "Epoch 47 Batch 150 Loss 0.7678 Accuracy 0.2429\n",
            "Epoch 47 Batch 200 Loss 0.7658 Accuracy 0.2423\n",
            "Epoch 47 Batch 250 Loss 0.7648 Accuracy 0.2421\n",
            "Epoch 47 Batch 300 Loss 0.7637 Accuracy 0.2426\n",
            "Epoch 47 Batch 350 Loss 0.7629 Accuracy 0.2427\n",
            "Epoch 47 Batch 400 Loss 0.7620 Accuracy 0.2425\n",
            "Epoch 47 Batch 450 Loss 0.7623 Accuracy 0.2425\n",
            "Epoch 47 Loss 0.7608 Accuracy 0.2426\n",
            "Time taken for 1 epoch: 69.00431251525879 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.8092 Accuracy 0.2437\n",
            "Epoch 48 Batch 50 Loss 0.7512 Accuracy 0.2432\n",
            "Epoch 48 Batch 100 Loss 0.7494 Accuracy 0.2450\n",
            "Epoch 48 Batch 150 Loss 0.7502 Accuracy 0.2447\n",
            "Epoch 48 Batch 200 Loss 0.7477 Accuracy 0.2441\n",
            "Epoch 48 Batch 250 Loss 0.7473 Accuracy 0.2444\n",
            "Epoch 48 Batch 300 Loss 0.7471 Accuracy 0.2445\n",
            "Epoch 48 Batch 350 Loss 0.7473 Accuracy 0.2443\n",
            "Epoch 48 Batch 400 Loss 0.7459 Accuracy 0.2443\n",
            "Epoch 48 Batch 450 Loss 0.7455 Accuracy 0.2445\n",
            "Epoch 48 Loss 0.7438 Accuracy 0.2444\n",
            "Time taken for 1 epoch: 68.95747089385986 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.7367 Accuracy 0.2412\n",
            "Epoch 49 Batch 50 Loss 0.7434 Accuracy 0.2449\n",
            "Epoch 49 Batch 100 Loss 0.7371 Accuracy 0.2468\n",
            "Epoch 49 Batch 150 Loss 0.7379 Accuracy 0.2461\n",
            "Epoch 49 Batch 200 Loss 0.7354 Accuracy 0.2451\n",
            "Epoch 49 Batch 250 Loss 0.7354 Accuracy 0.2453\n",
            "Epoch 49 Batch 300 Loss 0.7326 Accuracy 0.2458\n",
            "Epoch 49 Batch 350 Loss 0.7327 Accuracy 0.2456\n",
            "Epoch 49 Batch 400 Loss 0.7323 Accuracy 0.2454\n",
            "Epoch 49 Batch 450 Loss 0.7311 Accuracy 0.2457\n",
            "Epoch 49 Loss 0.7282 Accuracy 0.2458\n",
            "Time taken for 1 epoch: 68.90398454666138 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.8049 Accuracy 0.2290\n",
            "Epoch 50 Batch 50 Loss 0.7207 Accuracy 0.2476\n",
            "Epoch 50 Batch 100 Loss 0.7177 Accuracy 0.2481\n",
            "Epoch 50 Batch 150 Loss 0.7155 Accuracy 0.2479\n",
            "Epoch 50 Batch 200 Loss 0.7149 Accuracy 0.2468\n",
            "Epoch 50 Batch 250 Loss 0.7137 Accuracy 0.2470\n",
            "Epoch 50 Batch 300 Loss 0.7136 Accuracy 0.2475\n",
            "Epoch 50 Batch 350 Loss 0.7127 Accuracy 0.2474\n",
            "Epoch 50 Batch 400 Loss 0.7112 Accuracy 0.2474\n",
            "Epoch 50 Batch 450 Loss 0.7090 Accuracy 0.2475\n",
            "Epoch 50 Loss 0.7075 Accuracy 0.2476\n",
            "Time taken for 1 epoch: 69.00541639328003 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.7538 Accuracy 0.2114\n",
            "Epoch 51 Batch 50 Loss 0.6989 Accuracy 0.2485\n",
            "Epoch 51 Batch 100 Loss 0.7035 Accuracy 0.2497\n",
            "Epoch 51 Batch 150 Loss 0.7015 Accuracy 0.2493\n",
            "Epoch 51 Batch 200 Loss 0.7020 Accuracy 0.2486\n",
            "Epoch 51 Batch 250 Loss 0.7018 Accuracy 0.2485\n",
            "Epoch 51 Batch 300 Loss 0.6999 Accuracy 0.2488\n",
            "Epoch 51 Batch 350 Loss 0.6980 Accuracy 0.2489\n",
            "Epoch 51 Batch 400 Loss 0.6973 Accuracy 0.2486\n",
            "Epoch 51 Batch 450 Loss 0.6977 Accuracy 0.2486\n",
            "Epoch 51 Loss 0.6962 Accuracy 0.2487\n",
            "Time taken for 1 epoch: 68.98702955245972 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.7700 Accuracy 0.2466\n",
            "Epoch 52 Batch 50 Loss 0.6869 Accuracy 0.2503\n",
            "Epoch 52 Batch 100 Loss 0.6858 Accuracy 0.2506\n",
            "Epoch 52 Batch 150 Loss 0.6862 Accuracy 0.2503\n",
            "Epoch 52 Batch 200 Loss 0.6832 Accuracy 0.2497\n",
            "Epoch 52 Batch 250 Loss 0.6824 Accuracy 0.2500\n",
            "Epoch 52 Batch 300 Loss 0.6801 Accuracy 0.2505\n",
            "Epoch 52 Batch 350 Loss 0.6797 Accuracy 0.2502\n",
            "Epoch 52 Batch 400 Loss 0.6788 Accuracy 0.2504\n",
            "Epoch 52 Batch 450 Loss 0.6782 Accuracy 0.2505\n",
            "Epoch 52 Loss 0.6764 Accuracy 0.2506\n",
            "Time taken for 1 epoch: 68.95584058761597 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.7595 Accuracy 0.2354\n",
            "Epoch 53 Batch 50 Loss 0.6840 Accuracy 0.2517\n",
            "Epoch 53 Batch 100 Loss 0.6783 Accuracy 0.2521\n",
            "Epoch 53 Batch 150 Loss 0.6740 Accuracy 0.2520\n",
            "Epoch 53 Batch 200 Loss 0.6708 Accuracy 0.2514\n",
            "Epoch 53 Batch 250 Loss 0.6712 Accuracy 0.2511\n",
            "Epoch 53 Batch 300 Loss 0.6717 Accuracy 0.2513\n",
            "Epoch 53 Batch 350 Loss 0.6698 Accuracy 0.2514\n",
            "Epoch 53 Batch 400 Loss 0.6686 Accuracy 0.2512\n",
            "Epoch 53 Batch 450 Loss 0.6682 Accuracy 0.2514\n",
            "Epoch 53 Loss 0.6663 Accuracy 0.2515\n",
            "Time taken for 1 epoch: 69.0140597820282 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.6078 Accuracy 0.2637\n",
            "Epoch 54 Batch 50 Loss 0.6585 Accuracy 0.2537\n",
            "Epoch 54 Batch 100 Loss 0.6556 Accuracy 0.2540\n",
            "Epoch 54 Batch 150 Loss 0.6538 Accuracy 0.2534\n",
            "Epoch 54 Batch 200 Loss 0.6569 Accuracy 0.2526\n",
            "Epoch 54 Batch 250 Loss 0.6568 Accuracy 0.2528\n",
            "Epoch 54 Batch 300 Loss 0.6550 Accuracy 0.2532\n",
            "Epoch 54 Batch 350 Loss 0.6549 Accuracy 0.2529\n",
            "Epoch 54 Batch 400 Loss 0.6543 Accuracy 0.2528\n",
            "Epoch 54 Batch 450 Loss 0.6537 Accuracy 0.2529\n",
            "Epoch 54 Loss 0.6535 Accuracy 0.2529\n",
            "Time taken for 1 epoch: 69.0293185710907 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.6474 Accuracy 0.2471\n",
            "Epoch 55 Batch 50 Loss 0.6439 Accuracy 0.2549\n",
            "Epoch 55 Batch 100 Loss 0.6411 Accuracy 0.2549\n",
            "Epoch 55 Batch 150 Loss 0.6447 Accuracy 0.2544\n",
            "Epoch 55 Batch 200 Loss 0.6443 Accuracy 0.2536\n",
            "Epoch 55 Batch 250 Loss 0.6438 Accuracy 0.2535\n",
            "Epoch 55 Batch 300 Loss 0.6421 Accuracy 0.2540\n",
            "Epoch 55 Batch 350 Loss 0.6426 Accuracy 0.2539\n",
            "Epoch 55 Batch 400 Loss 0.6412 Accuracy 0.2538\n",
            "Epoch 55 Batch 450 Loss 0.6404 Accuracy 0.2540\n",
            "Epoch 55 Loss 0.6395 Accuracy 0.2539\n",
            "Time taken for 1 epoch: 68.99328446388245 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.6977 Accuracy 0.2476\n",
            "Epoch 56 Batch 50 Loss 0.6357 Accuracy 0.2542\n",
            "Epoch 56 Batch 100 Loss 0.6315 Accuracy 0.2560\n",
            "Epoch 56 Batch 150 Loss 0.6338 Accuracy 0.2557\n",
            "Epoch 56 Batch 200 Loss 0.6324 Accuracy 0.2550\n",
            "Epoch 56 Batch 250 Loss 0.6310 Accuracy 0.2552\n",
            "Epoch 56 Batch 300 Loss 0.6302 Accuracy 0.2555\n",
            "Epoch 56 Batch 350 Loss 0.6299 Accuracy 0.2553\n",
            "Epoch 56 Batch 400 Loss 0.6295 Accuracy 0.2552\n",
            "Epoch 56 Batch 450 Loss 0.6282 Accuracy 0.2554\n",
            "Epoch 56 Loss 0.6259 Accuracy 0.2554\n",
            "Time taken for 1 epoch: 69.07575941085815 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.7237 Accuracy 0.2275\n",
            "Epoch 57 Batch 50 Loss 0.6155 Accuracy 0.2571\n",
            "Epoch 57 Batch 100 Loss 0.6175 Accuracy 0.2578\n",
            "Epoch 57 Batch 150 Loss 0.6213 Accuracy 0.2569\n",
            "Epoch 57 Batch 200 Loss 0.6211 Accuracy 0.2557\n",
            "Epoch 57 Batch 250 Loss 0.6212 Accuracy 0.2559\n",
            "Epoch 57 Batch 300 Loss 0.6202 Accuracy 0.2562\n",
            "Epoch 57 Batch 350 Loss 0.6186 Accuracy 0.2562\n",
            "Epoch 57 Batch 400 Loss 0.6182 Accuracy 0.2560\n",
            "Epoch 57 Batch 450 Loss 0.6181 Accuracy 0.2560\n",
            "Epoch 57 Loss 0.6159 Accuracy 0.2560\n",
            "Time taken for 1 epoch: 69.01237654685974 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.6275 Accuracy 0.2329\n",
            "Epoch 58 Batch 50 Loss 0.6050 Accuracy 0.2575\n",
            "Epoch 58 Batch 100 Loss 0.6066 Accuracy 0.2584\n",
            "Epoch 58 Batch 150 Loss 0.6061 Accuracy 0.2576\n",
            "Epoch 58 Batch 200 Loss 0.6053 Accuracy 0.2568\n",
            "Epoch 58 Batch 250 Loss 0.6058 Accuracy 0.2571\n",
            "Epoch 58 Batch 300 Loss 0.6042 Accuracy 0.2577\n",
            "Epoch 58 Batch 350 Loss 0.6039 Accuracy 0.2574\n",
            "Epoch 58 Batch 400 Loss 0.6031 Accuracy 0.2574\n",
            "Epoch 58 Batch 450 Loss 0.6026 Accuracy 0.2574\n",
            "Epoch 58 Loss 0.6003 Accuracy 0.2577\n",
            "Time taken for 1 epoch: 69.00837683677673 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.6292 Accuracy 0.2539\n",
            "Epoch 59 Batch 50 Loss 0.5926 Accuracy 0.2598\n",
            "Epoch 59 Batch 100 Loss 0.5963 Accuracy 0.2594\n",
            "Epoch 59 Batch 150 Loss 0.5973 Accuracy 0.2590\n",
            "Epoch 59 Batch 200 Loss 0.5961 Accuracy 0.2579\n",
            "Epoch 59 Batch 250 Loss 0.5944 Accuracy 0.2584\n",
            "Epoch 59 Batch 300 Loss 0.5935 Accuracy 0.2586\n",
            "Epoch 59 Batch 350 Loss 0.5929 Accuracy 0.2586\n",
            "Epoch 59 Batch 400 Loss 0.5915 Accuracy 0.2587\n",
            "Epoch 59 Batch 450 Loss 0.5919 Accuracy 0.2585\n",
            "Epoch 59 Loss 0.5895 Accuracy 0.2588\n",
            "Time taken for 1 epoch: 69.03396010398865 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.6655 Accuracy 0.2271\n",
            "Epoch 60 Batch 50 Loss 0.5793 Accuracy 0.2605\n",
            "Epoch 60 Batch 100 Loss 0.5788 Accuracy 0.2615\n",
            "Epoch 60 Batch 150 Loss 0.5790 Accuracy 0.2608\n",
            "Epoch 60 Batch 200 Loss 0.5785 Accuracy 0.2597\n",
            "Epoch 60 Batch 250 Loss 0.5783 Accuracy 0.2601\n",
            "Epoch 60 Batch 300 Loss 0.5790 Accuracy 0.2601\n",
            "Epoch 60 Batch 350 Loss 0.5781 Accuracy 0.2600\n",
            "Epoch 60 Batch 400 Loss 0.5788 Accuracy 0.2600\n",
            "Epoch 60 Batch 450 Loss 0.5790 Accuracy 0.2599\n",
            "Epoch 60 Loss 0.5785 Accuracy 0.2597\n",
            "Time taken for 1 epoch: 69.0206036567688 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.6684 Accuracy 0.2534\n",
            "Epoch 61 Batch 50 Loss 0.5679 Accuracy 0.2612\n",
            "Epoch 61 Batch 100 Loss 0.5739 Accuracy 0.2609\n",
            "Epoch 61 Batch 150 Loss 0.5746 Accuracy 0.2609\n",
            "Epoch 61 Batch 200 Loss 0.5767 Accuracy 0.2604\n",
            "Epoch 61 Batch 250 Loss 0.5767 Accuracy 0.2606\n",
            "Epoch 61 Batch 300 Loss 0.5757 Accuracy 0.2606\n",
            "Epoch 61 Batch 350 Loss 0.5760 Accuracy 0.2605\n",
            "Epoch 61 Batch 400 Loss 0.5758 Accuracy 0.2605\n",
            "Epoch 61 Batch 450 Loss 0.5749 Accuracy 0.2604\n",
            "Epoch 61 Loss 0.5729 Accuracy 0.2605\n",
            "Time taken for 1 epoch: 69.01501107215881 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.5961 Accuracy 0.2578\n",
            "Epoch 62 Batch 50 Loss 0.5645 Accuracy 0.2625\n",
            "Epoch 62 Batch 100 Loss 0.5612 Accuracy 0.2630\n",
            "Epoch 62 Batch 150 Loss 0.5617 Accuracy 0.2621\n",
            "Epoch 62 Batch 200 Loss 0.5605 Accuracy 0.2619\n",
            "Epoch 62 Batch 250 Loss 0.5612 Accuracy 0.2615\n",
            "Epoch 62 Batch 300 Loss 0.5595 Accuracy 0.2620\n",
            "Epoch 62 Batch 350 Loss 0.5587 Accuracy 0.2620\n",
            "Epoch 62 Batch 400 Loss 0.5590 Accuracy 0.2619\n",
            "Epoch 62 Batch 450 Loss 0.5580 Accuracy 0.2619\n",
            "Epoch 62 Loss 0.5567 Accuracy 0.2619\n",
            "Time taken for 1 epoch: 69.05976629257202 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.5150 Accuracy 0.2632\n",
            "Epoch 63 Batch 50 Loss 0.5579 Accuracy 0.2622\n",
            "Epoch 63 Batch 100 Loss 0.5603 Accuracy 0.2626\n",
            "Epoch 63 Batch 150 Loss 0.5602 Accuracy 0.2625\n",
            "Epoch 63 Batch 200 Loss 0.5583 Accuracy 0.2618\n",
            "Epoch 63 Batch 250 Loss 0.5560 Accuracy 0.2620\n",
            "Epoch 63 Batch 300 Loss 0.5552 Accuracy 0.2624\n",
            "Epoch 63 Batch 350 Loss 0.5547 Accuracy 0.2622\n",
            "Epoch 63 Batch 400 Loss 0.5542 Accuracy 0.2621\n",
            "Epoch 63 Batch 450 Loss 0.5523 Accuracy 0.2624\n",
            "Epoch 63 Loss 0.5500 Accuracy 0.2625\n",
            "Time taken for 1 epoch: 68.92693400382996 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.6258 Accuracy 0.2500\n",
            "Epoch 64 Batch 50 Loss 0.5423 Accuracy 0.2646\n",
            "Epoch 64 Batch 100 Loss 0.5473 Accuracy 0.2638\n",
            "Epoch 64 Batch 150 Loss 0.5476 Accuracy 0.2633\n",
            "Epoch 64 Batch 200 Loss 0.5476 Accuracy 0.2625\n",
            "Epoch 64 Batch 250 Loss 0.5465 Accuracy 0.2629\n",
            "Epoch 64 Batch 300 Loss 0.5460 Accuracy 0.2629\n",
            "Epoch 64 Batch 350 Loss 0.5454 Accuracy 0.2631\n",
            "Epoch 64 Batch 400 Loss 0.5452 Accuracy 0.2628\n",
            "Epoch 64 Batch 450 Loss 0.5443 Accuracy 0.2631\n",
            "Epoch 64 Loss 0.5430 Accuracy 0.2632\n",
            "Time taken for 1 epoch: 68.98608875274658 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.5849 Accuracy 0.2661\n",
            "Epoch 65 Batch 50 Loss 0.5234 Accuracy 0.2668\n",
            "Epoch 65 Batch 100 Loss 0.5278 Accuracy 0.2664\n",
            "Epoch 65 Batch 150 Loss 0.5337 Accuracy 0.2647\n",
            "Epoch 65 Batch 200 Loss 0.5339 Accuracy 0.2643\n",
            "Epoch 65 Batch 250 Loss 0.5340 Accuracy 0.2643\n",
            "Epoch 65 Batch 300 Loss 0.5331 Accuracy 0.2645\n",
            "Epoch 65 Batch 350 Loss 0.5319 Accuracy 0.2645\n",
            "Epoch 65 Batch 400 Loss 0.5325 Accuracy 0.2642\n",
            "Epoch 65 Batch 450 Loss 0.5317 Accuracy 0.2642\n",
            "Epoch 65 Loss 0.5305 Accuracy 0.2643\n",
            "Time taken for 1 epoch: 69.01278138160706 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.5580 Accuracy 0.2554\n",
            "Epoch 66 Batch 50 Loss 0.5238 Accuracy 0.2647\n",
            "Epoch 66 Batch 100 Loss 0.5217 Accuracy 0.2669\n",
            "Epoch 66 Batch 150 Loss 0.5227 Accuracy 0.2660\n",
            "Epoch 66 Batch 200 Loss 0.5238 Accuracy 0.2652\n",
            "Epoch 66 Batch 250 Loss 0.5241 Accuracy 0.2650\n",
            "Epoch 66 Batch 300 Loss 0.5241 Accuracy 0.2654\n",
            "Epoch 66 Batch 350 Loss 0.5243 Accuracy 0.2651\n",
            "Epoch 66 Batch 400 Loss 0.5253 Accuracy 0.2649\n",
            "Epoch 66 Batch 450 Loss 0.5259 Accuracy 0.2648\n",
            "Epoch 66 Loss 0.5250 Accuracy 0.2648\n",
            "Time taken for 1 epoch: 69.0119948387146 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.6297 Accuracy 0.2388\n",
            "Epoch 67 Batch 50 Loss 0.5192 Accuracy 0.2663\n",
            "Epoch 67 Batch 100 Loss 0.5190 Accuracy 0.2667\n",
            "Epoch 67 Batch 150 Loss 0.5209 Accuracy 0.2662\n",
            "Epoch 67 Batch 200 Loss 0.5201 Accuracy 0.2658\n",
            "Epoch 67 Batch 250 Loss 0.5213 Accuracy 0.2654\n",
            "Epoch 67 Batch 300 Loss 0.5198 Accuracy 0.2658\n",
            "Epoch 67 Batch 350 Loss 0.5194 Accuracy 0.2657\n",
            "Epoch 67 Batch 400 Loss 0.5188 Accuracy 0.2656\n",
            "Epoch 67 Batch 450 Loss 0.5181 Accuracy 0.2657\n",
            "Epoch 67 Loss 0.5166 Accuracy 0.2657\n",
            "Time taken for 1 epoch: 81.91752552986145 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.5124 Accuracy 0.2554\n",
            "Epoch 68 Batch 50 Loss 0.5199 Accuracy 0.2664\n",
            "Epoch 68 Batch 100 Loss 0.5121 Accuracy 0.2681\n",
            "Epoch 68 Batch 150 Loss 0.5119 Accuracy 0.2671\n",
            "Epoch 68 Batch 200 Loss 0.5101 Accuracy 0.2667\n",
            "Epoch 68 Batch 250 Loss 0.5106 Accuracy 0.2668\n",
            "Epoch 68 Batch 300 Loss 0.5105 Accuracy 0.2666\n",
            "Epoch 68 Batch 350 Loss 0.5090 Accuracy 0.2667\n",
            "Epoch 68 Batch 400 Loss 0.5090 Accuracy 0.2665\n",
            "Epoch 68 Batch 450 Loss 0.5086 Accuracy 0.2665\n",
            "Epoch 68 Loss 0.5073 Accuracy 0.2666\n",
            "Time taken for 1 epoch: 69.0335853099823 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.4941 Accuracy 0.2651\n",
            "Epoch 69 Batch 50 Loss 0.5046 Accuracy 0.2683\n",
            "Epoch 69 Batch 100 Loss 0.4993 Accuracy 0.2691\n",
            "Epoch 69 Batch 150 Loss 0.5008 Accuracy 0.2683\n",
            "Epoch 69 Batch 200 Loss 0.4994 Accuracy 0.2677\n",
            "Epoch 69 Batch 250 Loss 0.5005 Accuracy 0.2675\n",
            "Epoch 69 Batch 300 Loss 0.5001 Accuracy 0.2673\n",
            "Epoch 69 Batch 350 Loss 0.4984 Accuracy 0.2675\n",
            "Epoch 69 Batch 400 Loss 0.4981 Accuracy 0.2675\n",
            "Epoch 69 Batch 450 Loss 0.4980 Accuracy 0.2676\n",
            "Epoch 69 Loss 0.4972 Accuracy 0.2675\n",
            "Time taken for 1 epoch: 68.98452568054199 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.5278 Accuracy 0.2656\n",
            "Epoch 70 Batch 50 Loss 0.4953 Accuracy 0.2689\n",
            "Epoch 70 Batch 100 Loss 0.4973 Accuracy 0.2691\n",
            "Epoch 70 Batch 150 Loss 0.4967 Accuracy 0.2682\n",
            "Epoch 70 Batch 200 Loss 0.4950 Accuracy 0.2677\n",
            "Epoch 70 Batch 250 Loss 0.4950 Accuracy 0.2678\n",
            "Epoch 70 Batch 300 Loss 0.4949 Accuracy 0.2678\n",
            "Epoch 70 Batch 350 Loss 0.4950 Accuracy 0.2678\n",
            "Epoch 70 Batch 400 Loss 0.4935 Accuracy 0.2678\n",
            "Epoch 70 Batch 450 Loss 0.4929 Accuracy 0.2680\n",
            "Epoch 70 Loss 0.4915 Accuracy 0.2680\n",
            "Time taken for 1 epoch: 68.99703621864319 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pbj0bdG1JTv"
      },
      "source": [
        "transformer.save_weights('image_caption_transformer70_resnet50.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5r6nWrh1D5r"
      },
      "source": [
        "def evaluate(image):\n",
        "\n",
        "  temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "  img_tensor_val = image_features_extract_model(temp_input)\n",
        "  img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "  \n",
        "  start_token = tokenizer.word_index['<start>']\n",
        "  end_token = tokenizer.word_index['<end>']\n",
        "   \n",
        "  #decoder input is start token.\n",
        "  decoder_input = [start_token]\n",
        "  output = tf.expand_dims(decoder_input, 0) #tokens\n",
        "  result = [] #word list\n",
        "\n",
        "  for i in range(100):\n",
        "      dec_mask = create_masks_decoder(output)\n",
        "  \n",
        "      # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "      predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n",
        "      \n",
        "      # select the last word from the seq_len dimension\n",
        "      predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "      # return the result if the predicted_id is equal to the end token\n",
        "      if predicted_id == end_token:\n",
        "          return result,tf.squeeze(output, axis=0), attention_weights\n",
        "      # concatentate the predicted_id to the output which is given to the decoder\n",
        "      # as its input.\n",
        "      result.append(tokenizer.index_word[int(predicted_id)])\n",
        "      output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return result,tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkQuofIR3XXy"
      },
      "source": [
        "start_token = tokenizer.word_index['<start>']\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "# select random image from validation data\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "caption,result,attention_weights = evaluate(image)\n",
        "\n",
        "first = real_caption.split(' ', 1)[1]\n",
        "real_caption = first.rsplit(' ', 1)[0]\n",
        "\n",
        "#remove \"<unk>\" in result\n",
        "for i in caption:\n",
        "    if i==\"<unk>\":\n",
        "        caption.remove(i)\n",
        "\n",
        "for i in real_caption:\n",
        "    if i==\"<unk>\":\n",
        "        real_caption.remove(i)\n",
        "\n",
        "#remove <end> from result         \n",
        "result_join = ' '.join(caption)\n",
        "result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = caption\n",
        "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "print(f\"BLEU-1 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "print(f\"BLEU-2 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "print(f\"BLEU-3 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "print(f\"BLEU-4 score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Predicted Caption:', ' '.join(caption))\n",
        "temp_image = np.array(Image.open(image))\n",
        "plt.imshow(temp_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvUa6VCHSEzd"
      },
      "source": [
        "start_token = tokenizer.word_index['<start>']\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "# select random image from validation data\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "caption,result,attention_weights = evaluate(image)\n",
        "\n",
        "first = real_caption.split(' ', 1)[1]\n",
        "real_caption = first.rsplit(' ', 1)[0]\n",
        "\n",
        "#remove \"<unk>\" in result\n",
        "for i in caption:\n",
        "    if i==\"<unk>\":\n",
        "        caption.remove(i)\n",
        "\n",
        "for i in real_caption:\n",
        "    if i==\"<unk>\":\n",
        "        real_caption.remove(i)\n",
        "\n",
        "#remove <end> from result         \n",
        "result_join = ' '.join(caption)\n",
        "result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = caption\n",
        "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "print(f\"BLEU-1 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "print(f\"BLEU-2 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "print(f\"BLEU-3 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "print(f\"BLEU-4 score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Predicted Caption:', ' '.join(caption))\n",
        "temp_image = np.array(Image.open(image))\n",
        "plt.imshow(temp_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG64jntxSUIf"
      },
      "source": [
        "start_token = tokenizer.word_index['<start>']\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "# select random image from validation data\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "caption,result,attention_weights = evaluate(image)\n",
        "\n",
        "first = real_caption.split(' ', 1)[1]\n",
        "real_caption = first.rsplit(' ', 1)[0]\n",
        "\n",
        "#remove \"<unk>\" in result\n",
        "for i in caption:\n",
        "    if i==\"<unk>\":\n",
        "        caption.remove(i)\n",
        "\n",
        "for i in real_caption:\n",
        "    if i==\"<unk>\":\n",
        "        real_caption.remove(i)\n",
        "\n",
        "#remove <end> from result         \n",
        "result_join = ' '.join(caption)\n",
        "result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = caption\n",
        "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "print(f\"BLEU-1 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "print(f\"BLEU-2 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "print(f\"BLEU-3 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "print(f\"BLEU-4 score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Predicted Caption:', ' '.join(caption))\n",
        "temp_image = np.array(Image.open(image))\n",
        "plt.imshow(temp_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcAWvXQ2SUQI"
      },
      "source": [
        "start_token = tokenizer.word_index['<start>']\n",
        "end_token = tokenizer.word_index['<end>']\n",
        "# select random image from validation data\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "caption,result,attention_weights = evaluate(image)\n",
        "\n",
        "first = real_caption.split(' ', 1)[1]\n",
        "real_caption = first.rsplit(' ', 1)[0]\n",
        "\n",
        "#remove \"<unk>\" in result\n",
        "for i in caption:\n",
        "    if i==\"<unk>\":\n",
        "        caption.remove(i)\n",
        "\n",
        "for i in real_caption:\n",
        "    if i==\"<unk>\":\n",
        "        real_caption.remove(i)\n",
        "\n",
        "#remove <end> from result         \n",
        "result_join = ' '.join(caption)\n",
        "result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = caption\n",
        "score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "print(f\"BLEU-1 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "print(f\"BLEU-2 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "print(f\"BLEU-3 score: {score*100}\")\n",
        "score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "print(f\"BLEU-4 score: {score*100}\")\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Predicted Caption:', ' '.join(caption))\n",
        "temp_image = np.array(Image.open(image))\n",
        "plt.imshow(temp_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blue_1 = 0\n",
        "blue_2 = 0\n",
        "blue_3 = 0\n",
        "blue_4 = 0\n",
        "blue_count_1 = 0\n",
        "blue_count_2 = 0\n",
        "blue_count_3 = 0\n",
        "blue_count_4 = 0\n",
        "for j in range(250):\n",
        "    start_token = tokenizer.word_index['<start>']\n",
        "    end_token = tokenizer.word_index['<end>']\n",
        "    # select random image from validation data\n",
        "    rid = np.random.randint(0, len(img_name_val))\n",
        "    image = img_name_val[rid]\n",
        "    temp_image = np.array(Image.open(image))\n",
        "    plt.imshow(temp_image)\n",
        "    plt.show()\n",
        "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "    caption,result,attention_weights = evaluate(image)\n",
        "    first = real_caption.split(' ', 1)[1]\n",
        "    real_caption = first.rsplit(' ', 1)[0]\n",
        "\n",
        "    #remove \"<unk>\" in result\n",
        "    for i in caption:\n",
        "        if i==\"<unk>\":\n",
        "            caption.remove(i)\n",
        "\n",
        "    for i in real_caption:\n",
        "        if i==\"<unk>\":\n",
        "            real_caption.remove(i)\n",
        "\n",
        "    #remove <end> from result         \n",
        "    result_join = ' '.join(caption)\n",
        "    result_final = result_join.rsplit(' ', 1)[0]\n",
        "\n",
        "    real_appn = []\n",
        "    real_appn.append(real_caption.split())\n",
        "    reference = real_appn\n",
        "    candidate = caption\n",
        "    print ('Real Caption:', real_caption)\n",
        "    print ('Predicted Caption:', ' '.join(caption))\n",
        "    score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n",
        "    if score > 0.1:\n",
        "        blue_1 +=score\n",
        "        blue_count_1 +=1\n",
        "    print(f\"BLEU-1 score: {score*100}\")\n",
        "    score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n",
        "    if score > 0.1:\n",
        "        blue_2 +=score\n",
        "        blue_count_2 +=1\n",
        "    print(f\"BLEU-2 score: {score*100}\")\n",
        "    score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n",
        "    if score > 0.1:\n",
        "        blue_3 +=score\n",
        "        blue_count_3 += 1\n",
        "    print(f\"BLEU-3 score: {score*100}\")\n",
        "    score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n",
        "    if score > 0.1:\n",
        "        blue_4 +=score\n",
        "        blue_count_4 += 1\n",
        "    print(f\"BLEU-4 score: {score*100}\")\n",
        "\n",
        "print(f\"BLEU-1 count: {blue_count_1}\")\n",
        "print(f\"BLEU-2 count: {blue_count_2}\")\n",
        "print(f\"BLEU-3 count: {blue_count_3}\")\n",
        "print(f\"BLEU-4 count: {blue_count_4}\")\n",
        "print(f\"BLEU-1 Total: {blue_1}\")\n",
        "print(f\"BLEU-2 Total: {blue_2}\")\n",
        "print(f\"BLEU-3 Total: {blue_3}\")\n",
        "print(f\"BLEU-4 Total: {blue_4}\")\n",
        "if blue_count_1 > 0:\n",
        "    print(f\"BLEU-1 Average: {blue_1/blue_count_1}\")\n",
        "if blue_count_2 > 0:\n",
        "    print(f\"BLEU-2 Average: {blue_2/blue_count_2}\")\n",
        "if blue_count_3 > 0:\n",
        "    print(f\"BLEU-3 Average: {blue_3/blue_count_3}\")\n",
        "if blue_count_4 > 0:\n",
        "    print(f\"BLEU-4 Average: {blue_4/blue_count_4}\")\n",
        "\"\"\"\n",
        "print(f\"BLEU-1 count: {blue_count_1}\", f\"BLEU-1 Average: {blue_1/blue_count_1}\")\n",
        "print(f\"BLEU-1 count: {blue_count_2}\",f\"BLEU-1 Average: {blue_2/blue_count_2}\")\n",
        "print(f\"BLEU-1 count: {blue_count_3}\",f\"BLEU-1 Average: {blue_3/blue_count_3}\")\n",
        "print(f\"BLEU-1 count: {blue_count_4}\",f\"BLEU-1 Average: {blue_4/blue_count_4}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2CieR3KoD80T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}